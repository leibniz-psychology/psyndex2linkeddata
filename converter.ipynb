{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star2BF - Star to Bibframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Literal\n",
    "from rdflib.namespace import RDF, RDFS, Namespace\n",
    "# from rdflib.namespace import SCHEMA, XSD\n",
    "from rdflib import BNode\n",
    "import xml.etree.ElementTree as ET\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an \"element tree\" from the records in my xml file so we can loop through them and do things with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.parse(\"xml-data/records-440.xml\")\n",
    "\n",
    "# To see the source xml's structure, uncomment this function:\n",
    "# def print_element(element, depth=0):\n",
    "#     print(\"\\t\"*depth, element.tag, element.attrib, element.text)\n",
    "#     for child in element:\n",
    "#         print_element(child, depth+1)\n",
    "\n",
    "# for child in root.getroot()[:2]:\n",
    "#     print_element(child)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set a few namespace objects for bibframe, schema.org and for our resources (the works and instances) \n",
    "themselves.\n",
    "\n",
    "Then, we create two graphs from the xml source file, one to generate triples for our bibframe profile output, and the other for the simplified schema.org profile. \n",
    "\n",
    "Finally, we bind the prefixes with their appropriate namespaces to the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF = Namespace(\"http://id.loc.gov/ontologies/bibframe/\")\n",
    "BFLC = Namespace(\"http://id.loc.gov/ontologies/bflc/\")\n",
    "SCHEMA = Namespace(\"http://schema.org/\")\n",
    "WORKS = Namespace(\"https://w3id.org/zpid/resources/works/\")\n",
    "INSTANCES = Namespace(\"https://w3id.org/zpid/resources/instances/\")\n",
    "PXC = Namespace(\"https://w3id.org/zpid/ontology/classes/\")\n",
    "PXP = Namespace(\"https://w3id.org/zpid/ontology/properties/\")\n",
    "LANG = Namespace (\"http://id.loc.gov/vocabulary/iso639-2/\")\n",
    "\n",
    "# graph for bibframe profile:\n",
    "records_bf = Graph()\n",
    "\n",
    "# we need a new graph for the schema.org profile, so it won't just reuse the old triples from the other profile\n",
    "records_schema = Graph()\n",
    "\n",
    "# Bind the namespaces to the prefixes we want to see in the output:\n",
    "records_bf.bind(\"bf\", BF) \n",
    "records_bf.bind(\"bflc\", BFLC) \n",
    "records_bf.bind(\"works\", WORKS)  \n",
    "records_schema.bind(\"works\", WORKS) \n",
    "records_bf.bind(\"instances\", INSTANCES) \n",
    "records_bf.bind(\"pxc\", PXC) \n",
    "records_bf.bind(\"pxp\", PXP) \n",
    "records_bf.bind(\"lang\", LANG) \n",
    "records_schema.bind(\"instances\", INSTANCES) \n",
    "# todo: find out why the output uses \"schema1\" instead of \"schema\" for the schema.org namespace:\n",
    "records_schema.bind(\"schema\", SCHEMA, override=True) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to do all the things\n",
    "\n",
    "We need functions for the different things we will do - to avoid one long monolith of a loop.\n",
    "\n",
    "This is where they will go. Examples: Create blank nodes for Idebtifiers, create nested contribution objects from disparate person entries in AUP, AUK, CS and COU fields, merge PAUP (psychauthor person names and ids) with the person's name in AUP...\n",
    "\n",
    "These functions will later be called at the bottom of this notebook, in a loop over all the xml records."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Adding DFK as an Identifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFK as id for Bibframe\n",
    "\n",
    "We want to add the DFK as a local bf:Identifier to the work (or instance?). \n",
    "We also want to say where the Identifier originates (to say it is from PSYNDEX/ZPID). \n",
    "\n",
    "The format for that is:\n",
    "```turtle\n",
    "<Work/Instance> bf:identifiedBy [\n",
    "    a bf:Local, pxc:DFK; \n",
    "    rdf:value \"1234456\"; \n",
    "    bf:source [\n",
    "        a bf:Source; bf:code \"ZPID.PSYNDEX.DFK\"\n",
    "    ]\n",
    "];\n",
    "```\n",
    "\n",
    "So, we need a blank node for the Identifier and inside, another nested bnode for the bf:Source. This is a function that will return such an identifier bnode to add to the work_uri. We are calling it way up down below in the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a function to be called in a for-loop while going through all records of the source xml, \n",
    "# which returns a new triple to add to the graph that has a bnode for the dfk identifier.\n",
    "# The predicate is \"bf:identifiedBy\" and the object is a blank node of rdf:Type \"bf:Identifier\" and \"bf:Local\":\n",
    "# The actual identifier is a literal with the text from the \"DFK\" element of the record.\n",
    "def get_bf_identifier_dfk(dfk):\n",
    "    # make a  BNODE of the Identifier class from the BF namespace:\n",
    "    identifier = BNode()\n",
    "    identifier_source = BNode()\n",
    "    # records_bf.add ((identifier, RDF.type, BF.Identifier))\n",
    "    records_bf.add ((identifier, RDF.type, BF.Local))\n",
    "    records_bf.add ((identifier, RDF.type, PXC.DFK))\n",
    "    # build the source node:\n",
    "    records_bf.add((identifier_source, RDF.type, BF.Source))\n",
    "    records_bf.add((identifier_source, BF.code, Literal(\"ZPID.PSYNDEX.DFK\")))\n",
    "\n",
    "    # hang the id source node into the id node:\n",
    "    records_bf.add((identifier, BF.source, identifier_source))\n",
    "    records_bf.add((identifier, RDF.value, Literal(dfk)))\n",
    "    return (identifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Function: Replace languages with their language tag\n",
    "\n",
    "Can be used for different fields that are converted to langstrings or language uris. Use within other functions that work with the languages in different fields.\n",
    "\n",
    "Returns an array with two values: a two-letter langstring tag at [0] and a three-letter uri code for the library of congress language vocab at [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langtag_from_field(langfield):\n",
    "    # when passed a string from any language field in star, returns an array with two items. \n",
    "    # Index 0: two-letter langstring tag, e.g. \"de\"\n",
    "    # Index 1: two-letter iso langtag, e.g. \"ger\"\n",
    "    # can be used on these fields (it contains the different spellings found in them):\n",
    "    # \"LA\", \"LA2\", \"TIL\", \"TIUL\", \"ABLH\", \"ABLN\", \"TIUE |s\"\n",
    "    match langfield:\n",
    "        case \"german\" | \"de\" | \"GERM\" | \"Deutsch\" | \"GERMAN\" | \"GERMaN\" | \"German\" | \"Fi\":\n",
    "            return [\"de\", \"ger\"]\n",
    "        case \"en\" | \"ENGL\" | \"ENGLISH\" | \"Englisch\" | \"English\" | \"English; English\" | \"english\" :\n",
    "            return [\"en\", \"eng\"]\n",
    "        case \"BULG\" | \"Bulgarian\":\n",
    "            return [\"bg\", \"bul\"]\n",
    "        case \"SPAN\"| \"Spanish\":\n",
    "            return [\"es\", \"spa\"]\n",
    "        case \"Dutch\":\n",
    "            return [\"nl\", \"dut\"]\n",
    "        case \"CZEC\":\n",
    "            return [\"cs\", \"ces\"]\n",
    "        case \"FREN\" | \"French\":\n",
    "            return [\"fr\", \"fra\"]\n",
    "        case \"ITAL\" | \"Italian\":\n",
    "            return [\"it\", \"ita\"]\n",
    "        case \"PORT\" | \"Portuguese\":\n",
    "            return [\"pt\", \"por\"]\n",
    "        case \"JAPN\" | \"Japanese\":\n",
    "            return [\"jp\", \"jpn\"]\n",
    "        case \"HUNG\":\n",
    "            return [\"hu\", \"hun\"]\n",
    "        case \"RUSS\" | \"Russian\":\n",
    "            return [\"ru\", \"rus\"]\n",
    "        case \"NONE\" | \"Silent\":\n",
    "            return [\"zxx\", \"zxx\"]\n",
    "        case _:\n",
    "            return [\"und\", \"und\"] # for \"undetermined!\"\n",
    "\n",
    "# ---\n",
    "# these are also in those fields, but they are errors that should be repaired before migration!\n",
    "# X$English \n",
    "# EnglishX$\n",
    "# EnglishX$X$\n",
    "# $English\n",
    " \n",
    "# $German \n",
    "# GermanX$X$\n",
    "# X$$German\n",
    "# X$$GermanX$$German\n",
    "# GermanX$\n",
    "# GermanX$$EnglishX$$English\n",
    "# GermanX$$EnglishX$$EnglishX$$English\n",
    "# GermanX$English \n",
    "# GermanX$English X$English \n",
    "# GermanX$English X$English X$English \n",
    "# GermanX$English X$EnglishX$EnglishX$English\n",
    "# GermanX$English; English\n",
    "# GermanX$EnglishX$English\n",
    "# GermanX$EnglishX$EnglishX$English\n",
    "# GermanX$EnglishX$EnglishX$EnglishX$EnglishX$English\n",
    "# Fi (aus TIUL) - während TIL \"German\" ist! Das Dokument ist auch eindeutig Deutsch.\n",
    "\n",
    "# 4 (aus LA2) -> kann gelöscht werden, das Dokument ist in Deutsch und hat keine Zweitsprache!\n",
    "# Q (aus LA2) -> kann gelöscht werden, das Dokument ist in Deutsch und hat keine Zweitsprache! \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Get work language from LA\n",
    "\n",
    "Example\n",
    "\n",
    "```turtle\n",
    "@prefix lang: <http://id.loc.gov/vocabulary/iso639-2/> .\n",
    "<W> bf:language lang:ger .\n",
    "```\n",
    "\n",
    "Calls the generic language code lookup function above, get_langtag_from_field, passing the LA field content, returning a uri from the library of congress language vocabulary (built from namespace + 3-letter iso code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function \n",
    "def get_work_language(record):\n",
    "    work_language = get_langtag_from_field(record.find(\"LA\").text.strip())[1]\n",
    "    work_lang_uri = LANG[work_language]\n",
    "    return (work_lang_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create Instance Title nodes from fields TI, TIU, TIL, TIUE...\n",
    "\n",
    "Titles and Translated titles are attached to Instances. Translated titles also have a source, which can be DeepL, ZPID, or Original.\n",
    "\n",
    "Example:\n",
    "\n",
    "```turtle\n",
    "<Instance> bf:title \n",
    "        [a bf:Title; \n",
    "            bf:mainTitle \"Disentangling the process of epistemic change\"@en;\n",
    "            bf:subtitle \"The role of epistemic volition\"@en;\n",
    "        ],\n",
    "        [a pxc:TranslatedTitle;\n",
    "            rdfs:label \"Den Prozess des epistemischen Wandels entwirren: Die Rolle des epistemischen Willens.\"@de;\n",
    "            bf:mainTitle \"Den Prozess des epistemischen Wandels entwirren: Die Rolle des epistemischen Willens.\"@de;\n",
    "            bf:adminMetadata  [ \n",
    "                a bf:AdminMetadata ;\n",
    "                bflc:metadataLicensor  \"DeepL\";\n",
    "        ]\n",
    "        ].\n",
    "```\n",
    "\n",
    "- [x] add TI as bf:Title via bf:mainTitle\n",
    "- [x] add subtitle from TIU\n",
    "- [x] create a concatenated rdfs:label from TI and TIU\n",
    "- [x] add languages for maintitle and subtitle (from TIL and TIUL)\n",
    "\n",
    "- [x] add translated title from TIUE as pxc:TranslatedTitle with bf:mainTitle and rdfs:label \n",
    "- [x] add languages for translated title (from subfield TIU |s, or if unavailable, decide language based on TIL language: if de -> en and vice versa) \n",
    "- [x] find a way to create a source for the translated title (from \"(DeepL)\" at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a function to be called in a for-loop while going through all records of the source xml, \n",
    "# which returns a new triple to add to the graph that has a bnode for the dfk identifier.\n",
    "# The predicate is \"bf:identifiedBy\" and the object is a blank node of rdf:Type \"bf:Identifier\" and \"bf:Local\":\n",
    "# The actual identifier is a literal with the text from the \"DFK\" element of the record.\n",
    "def get_bf_title(record):\n",
    "    # make a  BNODE for the title:\n",
    "    title = BNode()\n",
    "    # make it bf:Title class:\n",
    "    records_bf.add ((title, RDF.type, BF.Title))\n",
    "\n",
    "    # get the content of th TI field as the main title:\n",
    "    maintitle = record.find(\"TI\").text.strip()\n",
    "    # write a full title for the rdfs:label \n",
    "    # (update later if subtitle exists to add that)\n",
    "    fulltitle = maintitle\n",
    "    # set dafault language for main title:\n",
    "    maintitle_language = \"en\"\n",
    "    subtitle_language = \"en\"\n",
    "    # get language of main title - if exists!:\n",
    "    if record.find(\"TIL\") is not None:\n",
    "        maintitle_language = get_langtag_from_field(record.find(\"TIL\").text.strip())[0]\n",
    "        # if maintitle_language_til == \"German\":\n",
    "        #     maintitle_language = \"de\"\n",
    "        # else: just keep the default set above: \"en\"\n",
    "    # get language of subtitle:\n",
    "    if record.find(\"TIUL\") is not None:\n",
    "        subtitle_language = get_langtag_from_field(record.find(\"TIUL\").text.strip())[0]\n",
    "        # subtitle_language_tiul = record.find(\"TIUL\").text.strip()\n",
    "        # if subtitle_language_tiul == \"German\":\n",
    "        #     subtitle_language = \"de\"\n",
    "        # else: just keep the default set above: \"en\"\n",
    "\n",
    "    # add the content of TI etc via bf:mainTitle:\n",
    "    records_bf.add((title, BF.mainTitle, Literal(maintitle, lang=maintitle_language)))\n",
    "    # get content of the TIU field as the subtitle, \n",
    "    # _if_ it exists and has text in it:\n",
    "    if record.find(\"TIU\") is not None and record.find(\"TIU\") != \"\":\n",
    "        subtitle = record.find(\"TIU\").text.strip() # remove extraneous spaces\n",
    "        # concatenate a full title from main- and subtitle, \n",
    "        # separated with a : and overwrite fulltitle with that\n",
    "        fulltitle = fulltitle + \": \" + subtitle\n",
    "        # add the content of TIU to the bf:Title via bf:subtitle:\n",
    "        records_bf.add((title, BF.subtitle, Literal(subtitle, lang=subtitle_language)))\n",
    "\n",
    "    # add the concatenated full title to the bf:Title via rdfs:label:\n",
    "    # (we don't care if the main title's and subtitle's languages don't match - we just set the language of the main title as the full title's language)\n",
    "    records_bf.add((title, RDFS.label, Literal(fulltitle, lang=maintitle_language)))\n",
    "\n",
    "    # # hang the id source node into the id node:\n",
    "    # records_bf.add((identifier, BF.source, identifier_source))\n",
    "    return (title)\n",
    "\n",
    "# function for the translated title:\n",
    "def get_bf_translated_title(record):\n",
    "    translated_title = BNode()\n",
    "    records_bf.add ((translated_title, RDF.type, PXC.TranslatedTitle))\n",
    "    fulltitle = record.find(\"TIUE\").text.strip()\n",
    "    fulltitle_language = \"de\"\n",
    "    # TODO: find a way to read subfield |s to get the actual language. \n",
    "    # it that doesn't exist, use the inverse of TIL!\n",
    "    # for now we just default to German (de)\n",
    "\n",
    "    # if fulltitle string ends with \"|s \" followed by some text (use a regex):\n",
    "    match = re.search(r'^(.*)\\s\\|s\\s(.*)', fulltitle)\n",
    "    if match:\n",
    "        fulltitle = match.group(1).strip()\n",
    "        fulltitle_language = get_langtag_from_field(match.group(2).strip())[0]\n",
    "    else:\n",
    "        # get the language in TIUE, if that field exists\n",
    "        if record.find(\"TIL\") is not None:\n",
    "            original_title_language_til = get_langtag_from_field(record.find(\"TIL\").text.strip())[0]\n",
    "            \n",
    "            # if it is German -> use inverse: \"en\"\n",
    "            if original_title_language_til == \"de\":\n",
    "                fulltitle_language = \"en\"\n",
    "            # else -> keep \"de\"\n",
    "\n",
    "    # check if the title contains a \"(DeepL)\" and cut it into a variable for the source:\n",
    "    titlesource = \"ZPID\" # translation source is \"ZPID\" by default\n",
    "    # note: we might be able to add source \"Original\" by finding out \n",
    "    # if the source of the secondary abstract is something other than ZPID!\n",
    "    match_source = re.search(r'^(.*)\\((DeepL)\\)$', fulltitle)\n",
    "    if match_source:\n",
    "        fulltitle = match_source.group(1).strip()\n",
    "        titlesource = match_source.group(2)\n",
    "\n",
    "    # build a source node for the translation:\n",
    "    titlesource_node = BNode ()\n",
    "    records_bf.add ((titlesource_node, RDF.type, BF.AdminMetadata))\n",
    "    records_bf.add ((titlesource_node, BFLC.metadataLicensor, Literal(titlesource)))\n",
    "\n",
    "    # add the title string to the bnode:\n",
    "    records_bf.add((translated_title, BF.mainTitle, Literal(fulltitle, lang=fulltitle_language)))\n",
    "    records_bf.add((translated_title, RDFS.label, Literal(fulltitle, lang=fulltitle_language)))\n",
    "    records_bf.add((translated_title, BF.adminMetadata, titlesource_node))\n",
    "\n",
    "    return (translated_title)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Function: Add Abstracts - original abstract (from fields ABH, ABLH, ABSH1, ABSH2) and translated/secondary abstract (from ABN, ABLN, ASN1, ASN2)\n",
    "\n",
    "- Main Abstract: \n",
    "    - abstract text is in field ABH.\n",
    "    - abstract language is in ABLH (\"German\" or \"English\")\n",
    "    - abstract original source is in ASH1 (\"Original\" or \"ZPID\")\n",
    "    - agent who edited the original, if that happened, is in ASH2 ()\n",
    "- Secondary Abstract \n",
    "    - abstract text is in field ABN.\n",
    "    - abstract language is in ABLN (\"German\" or \"English\")\n",
    "    - abstract original source is in ASN1 (\"Original\" or \"ZPID\")\n",
    "    - agent who edited the original, if that happened, is in ASN2 ()\n",
    "\n",
    "Use this scheme:\n",
    "\n",
    "```turtle\n",
    "<W> bf:summary \n",
    "    [ a pxc:Abstract , bf:Summary ;\n",
    "        rdfs:label  \"Background: Loneliness is ...\"@en ;\n",
    "        bf:adminMetadata  [ \n",
    "            a bf:AdminMetadata ;\n",
    "            bflc:metadataLicensor  \"Original\";\n",
    "            bf:descriptionModifier \"ZPID\"\n",
    "        ]\n",
    "] .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the original abstract:\n",
    "def get_bf_abstract(record):\n",
    "    abstract = BNode()\n",
    "    records_bf.add ((abstract, RDF.type, PXC.Abstract))\n",
    "    # get abstract text from ABH\n",
    "    abstracttext = record.find(\"ABH\").text.strip()\n",
    "    # get abstract language from ABLH (\"German\" or \"English\")\n",
    "    abstract_language = \"en\" # set default\n",
    "    if record.find(\"ABLH\") is not None:\n",
    "        abstract_language = get_langtag_from_field(record.find(\"ABLH\").text.strip())[0]\n",
    "\n",
    "    # add the text to the bnode:\n",
    "    records_bf.add ((abstract, RDFS.label, Literal(abstracttext, lang=abstract_language)))\n",
    "\n",
    "    # get abstract original source from ASH1 (\"Original\" or \"ZPID\")\n",
    "    abstract_source = \"Original\" # default\n",
    "    # create a blank node for admin metadata:\n",
    "    abstract_source_node = BNode()\n",
    "    records_bf.add((abstract_source_node, RDF.type, BF.AdminMetadata))\n",
    "\n",
    "    if record.find(\"ASH1\") is not None:\n",
    "        # overwrite default (\"Original\") with what we find in ASH1:\n",
    "        abstract_source = record.find(\"ASH1\").text.strip()\n",
    "    \n",
    "    # write final source text into source node:\n",
    "    records_bf.add((abstract_source_node, BFLC.metadataLicensor, Literal(abstract_source)))\n",
    "\n",
    "    # here is a list of known zpid employee tags, we will use them later to replace these with \"ZPID\" if found in ASH2:\n",
    "\n",
    "    # and this is a list of things we want to replace with \"Original\":\n",
    "    \n",
    "\n",
    "    # get optional agent who edited the original abstract from ASH2\n",
    "    if record.find(\"ASH2\") is not None:\n",
    "        # note what we find in ABSH2:\n",
    "        abstract_editor = record.find(\"ASH2\").text.strip()\n",
    "        # todo: replace known zpid person initials with \"ZPID\"\n",
    "        # \"Juergen Wiesenhuetter\",\n",
    "        # \"Joachim H. Becker\",\"Udo Wolff\", \"Juergen Beling\", \n",
    "        # \"Joachim H. Mueller\", \"Angelika Zimmer\", \"Annelie Wiertz\", \"Beate Minsel\", \"Berndt Zuschlag\",  \"Doris Lecheler\", \"Elke Bone\", \"Guenter Krampen\", \"Hella Lenders\", \"Jutta Rohlmann\", \"Juergen Howe\", \"Manfred Opitz\", \"Manfred Fischer\", \"Paul Klein\", \"Sigrun-Heide Filipp\", \"Thomas W. Franke\", \"Ulrike Fischer\", \"Yrla M. Labouvie\", \n",
    "        # \"K.Si\", \"L.F.T.\", \"M.G.\", \"I.D.\" , \"A.Bi.\", \"A.G.\", \"A.C.\", \"U.R.W\", \"U\", \"C.Si\", \"pe.k\", \"r\", \"R.N\", \"Ve.K.\",   \n",
    "\n",
    "        # if \"Author\" or \"Autor\" -> \"Original\"\n",
    "        # and what if \"DeepL\"???\n",
    "        # or \"FIS Bildung\", \"GESIS Fachinformation für die Sozialwissenschaften, Bonn\", \"Kriminologische Zentralstelle\", \n",
    "        # and add it via decription modifier:\n",
    "        records_bf.add((abstract_source_node, BF.descriptionModifier, Literal(abstract_editor)))\n",
    "\n",
    "\n",
    "    #add the source node to the abstract node:\n",
    "    records_bf.add((abstract, BF.adminMetadata, abstract_source_node))\n",
    "    # and return the completed node:\n",
    "    return (abstract)\n",
    "\n",
    "def get_bf_secondary_abstract(record):\n",
    "    abstract = BNode()\n",
    "    records_bf.add ((abstract, RDF.type, PXC.Abstract))\n",
    "    records_bf.add ((abstract, RDF.type, PXC.SecondaryAbstract))\n",
    "    abstracttext = record.find(\"ABN\").text.strip()\n",
    "    \n",
    "    abstract_language = \"de\" # fallback default\n",
    "    if record.find(\"ABLN\") is not None:\n",
    "        abstract_language = get_langtag_from_field(record.find(\"ABLN\").text.strip())[0]\n",
    "    \n",
    "    records_bf.add ((abstract, RDFS.label, Literal(abstracttext, lang=abstract_language)))\n",
    "    \n",
    "    abstract_source_node = BNode()\n",
    "    records_bf.add((abstract_source_node, RDF.type, BF.AdminMetadata))\n",
    "    abstract_source = \"Original\" # fallback default\n",
    "    if record.find(\"ASN1\") is not None:\n",
    "        # overwrite default (\"Original\") with what we find in ASH1:\n",
    "        abstract_source = record.find(\"ASN1\").text.strip()\n",
    "    \n",
    "    records_bf.add((abstract_source_node, BFLC.metadataLicensor, Literal(abstract_source)))\n",
    "\n",
    "    # get optional agent who edited the original abstract from ASH2\n",
    "    if record.find(\"ASN2\") is not None:\n",
    "        # note what we find in ABSN2:\n",
    "        abstract_editor = record.find(\"ASN2\").text.strip()\n",
    "        # and add it via decription modifier:\n",
    "        records_bf.add((abstract_source_node, BF.descriptionModifier, Literal(abstract_editor)))\n",
    "\n",
    "    #add the source node to the abstract node:\n",
    "    records_bf.add((abstract, BF.adminMetadata, abstract_source_node))\n",
    "    # and return the completed node:\n",
    "    return (abstract)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Function to split Table of Content from the Abstract field (ABH)\n",
    "\n",
    "This usually starts with \" - Inhalt: \" (for German Abstracts) or \" - Contents: \" (in English abstracts) and ends at the end of the field.\n",
    "It can contain a numbered list of chapters or sections as a long string. It can also contain a uri from dnb namespace instead or in addition!\n",
    "\n",
    "Examples:\n",
    "- \" - Contents: (1) ...\"\n",
    "- \" - Inhalt: https://d-nb.info/1256712809/04</ABH>\" (URI pattern: \"https://d-nb.info/\" + \"1256712809\" 10 digits + \"/04\")\n",
    "\n",
    "Example:\n",
    "\n",
    "```turtle\n",
    "<W> bf:tableOfContents [\n",
    "    a bf:TableOfContents;\n",
    "    rdfs:label \"(1) Wünsche, J., Weidmann, R. &amp; Grob, A. (n. d.). Happy in the same way? The link between domain satisfaction and overall life satisfaction in romantic couples. Manuscript submitted for publication. (2) Wünsche, J., Weidmann,...\";\n",
    "] .\n",
    "```\n",
    "\n",
    "Or\n",
    "\n",
    "```turtle\n",
    "<W> bf:tableOfContents [\n",
    "    a bf:TableOfContents;\n",
    "    rdf:value \"https://d-nb.info/1002790794/04\"^^xsd:anyURI ;\n",
    "] .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bf_toc(work_uri, record):\n",
    "    # read the abstract in ABH\n",
    "    contents = \"\"\n",
    "    if record.find(\"ABH\") is not None:\n",
    "        abstracttext = record.find(\"ABH\").text.strip()\n",
    "        # check via regex if there is a \" - Inhalt: \" or \" - Contents: \" in it.\n",
    "        # if so, split out what comes after. Drop the contents/inhalt part itself.\n",
    "        match = re.search(r'^(.*)[-–]\\s*(?:Contents|Inhalt)\\s*:\\s*(.*)$', abstracttext)\n",
    "        if match:\n",
    "            abstracttext = match.group(1).strip()\n",
    "            contents = match.group(2).strip()\n",
    "\n",
    "    # also check if what comes is either a string or a uri following thegiven pattern\n",
    "    # and export one as a rdfs_label and the other as rdf:value \"...\"^^xsd:anyUrl (remember to add XSD namespace!)\n",
    "    # also remember that we should only create a node and attach it to the work\n",
    "    # if a) ABH exists at all and\n",
    "    # b) the regex is satisfied.\n",
    "    # So I guess we must do the whole checking and adding procedure in this function!\n",
    "\n",
    "    # only return an added triple if the toc exisits, otherwise return nothing:\n",
    "    if contents:\n",
    "        return records_bf.add((work_uri, BF.tableOfContents, Literal(contents)))\n",
    "    else: \n",
    "        return None\n",
    "    # return records_bf.add((work_uri, BF.tableOfContents, Literal(\"test\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Function: Create Contribution nodes from Fields AUP, EMID, EMAIL, AUK, PAUP, CS and COU\n",
    "\n",
    "Use this scheme:\n",
    "\n",
    "```turtle\n",
    "<Work> a bf:Work;\n",
    "    bf:contribution \n",
    "    [\n",
    "        # the Bibframe Contribution includes, as usual, an agent and their role,\n",
    "        # but is supplemented with an Affiliation (in the context of that work/while it was written),\n",
    "        # and a position in the author sequence.\n",
    "        a bf:Contribution, bflc:PrimaryContribution; \n",
    "        bf:agent \n",
    "        [\n",
    "            a bf:Person, schema:Person; \n",
    "            rdfs:label \"Trillitzsch, Tina\"; # name when creating work\n",
    "            schema:givenName \"Tina\"; schema:familyName \"Trillitzsch\";\n",
    "            owl:sameAs <https://w3id.org/zpid/person/tt_0000001>, <https://orcid.org/0000-0001-7239-4844>; # authority uris of person (local, orcid)\n",
    "            bf:identifiedBy [a bf:Local, pxc:PsychAuthorsID; rdf:value \"p01979TTR\"; #legacy authority ID\n",
    "            ];\n",
    "            bf:identifiedBy [a bf:Identifier, locid:orcid; rdf:value \"0000-0001-7239-4844\"; # ORCID \n",
    "            ];\n",
    "        ]\n",
    "        # we use a model inspired by Option C in Osma Suominen'a suggestion for https://github.com/dcmi/dc-srap/issues/3\n",
    "        # adding the Affiliation into the Contribution, separate from the agent itself, since the affiliation\n",
    "        # is described in the context of this work, not not as a statement about the person's\n",
    "        # current affiliation:\n",
    "        mads:hasAffiliation [\n",
    "            a mads:Affiliation;\n",
    "            # Affiliation blank node has info about the affiliation org (including persistent identifiers),\n",
    "            # the address (country with geonames identifier),\n",
    "            # and the person's email while affiliated there.\n",
    "            mads:organization [\n",
    "                a bf:Organization; \n",
    "                rdfs:label \"Leibniz Institute of Psychology (ZPID); Digital Research Development Services\"; # org name when work was created\n",
    "                owl:sameAs <https://w3id.org/zpid/org/zpid_0000001>, <https://ror.org/0165gz615>; # authority uris of org (local, ror)\n",
    "                # internal id and ror id as literal identifiers:\n",
    "                bf:identifiedBy [a bf:Local, pxc:ZpidCorporateBodyId; rdf:value \"0000001\"; ];\n",
    "                bf:identifiedBy [a bf:Identifier; locid:ror; rdf:value \"0165gz615\"; ];\n",
    "            ];\n",
    "            mads:hasAffiliationAddress [a mads:Address;\n",
    "                mads:country [\n",
    "                    a mads:Country, bf:Place;\n",
    "                    rdfs:label \"Germany\";\n",
    "                    bf:identifiedBy [a bf:Identifier, locid:geonames; rdf:value \"2921044\"; ];\n",
    "                    owl:sameAs <https://w3id.org/zpid/place/country/ger>;\n",
    "                ]\n",
    "            ];\n",
    "            mads:email <mailto:ttr@leibniz-psychology.org>; # correspondence author email\n",
    "        ];\n",
    "        bf:role <http://id.loc.gov/vocabulary/relators/aut>;\n",
    "        pxp:contributionPosition 1; bf:qualifier \"first\"; # first author in sequence: our own subproperty of bf:qualifier & schema:position (also: middle, last)\n",
    "    ].\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Function: Create Topics, Weighted Topics and Classifications from CT, SH\n",
    "\n",
    "Use this scheme:\n",
    "\n",
    "```turtle\n",
    "<Work> a bf:Work;\n",
    "    bf:subject [a bf:Topic, pxc:WeightedTopic, skos:Concept; # # topic, weighted\n",
    "        owl:sameAs <https://w3id.org/zpid/vocabs/terms/35365>;\n",
    "        rdfs:label \"Ontologies\"@en, \"Ontologien\"@de;\n",
    "        bf:source <https://w3id.org/zpid/vocabs/terms>;\n",
    "    ];\n",
    "    bf:subject [a bf:Topic, skos:Concept; # a non-weighted topic\n",
    "        owl:sameAs <https://w3id.org/zpid/vocabs/terms/60135>;\n",
    "        rdfs:label \"Semantic Networks\"@en, \"Semantische Netzwerke\"@de;\n",
    "        bf:source <https://w3id.org/zpid/vocabs/terms>;\n",
    "    ];\n",
    "    # PSYNDEX subject heading classification\n",
    "    bf:classification [ a bf:Classification, pxc:SubjectHeading, skos:Concept;\n",
    "        rdfs:label \"Professional Psychological & Health Personnel Issues\"@en;\n",
    "        bf:code \"3400\";\n",
    "        owl:sameAs <https://w3id.org/zpid/vocabs/class/3400>;\n",
    "        bf:source <https://w3id.org/zpid/vocabs/class>;\n",
    "    ].\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Function: Create nodes for Population Age Group (AGE) and Population Location (PLOC)\n",
    "\n",
    "Use this scheme:\n",
    "\n",
    "```turtle\n",
    "<Work> \n",
    "# age group study is about/sample was from:\n",
    "    bflc:demographicGroup [a bflc:DemographicGroup, pxc:AgeGroup, skos:Concept;\n",
    "        rdfs:label \"Adulthood\"@en, \"Erwachsenenalter\"@de;\n",
    "        owl:sameAs <https://w3id.org/zpid/vocabs/age/adulthood>;\n",
    "        bf:source <https://w3id.org/zpid/vocabs/age/AgeGroups>; \n",
    "    ];\n",
    "    # population location: \n",
    "    bf:geographicCoverage [a bf:GeographicCoverage, pxc:PopulationLocation, skos:Concept;\n",
    "        rdfs:label \"Germany\"@en;\n",
    "        owl:sameAs <countries/ger>;\n",
    "    ].\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loop!\n",
    "## Creating the Work and Instance uris and adding other triples via functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uris and types for Bibframe profile\n",
    "\n",
    "We want two URIs, since we split the Records into (at first) one work and one instance, which will be linked together.\n",
    "We also say one will be a (rdf:type) bf:Work and the other bf:Instance.\n",
    "Then we print all these triples into a file for the bibframe profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9173 triples\n"
     ]
    }
   ],
   "source": [
    "# print(len(root.findall(\"Record\")))\n",
    "\n",
    "\n",
    "for record in root.findall(\"Record\"):\n",
    "\n",
    "    # get the DFK identifier from the record:\n",
    "    dfk = record.find(\"DFK\").text\n",
    "\n",
    "    # create a URI for the work and the instance and give them their correct bf classes:\n",
    "    work_uri = WORKS[dfk]\n",
    "    records_bf.add((work_uri, RDF.type, BF.Work))\n",
    "    instance_uri = INSTANCES[dfk]\n",
    "    records_bf.add((instance_uri, RDF.type, BF.Instance))\n",
    "\n",
    "    # connect work and instance via bf:instanceOf and bf:hasInstance:\n",
    "    records_bf.add((instance_uri, BF.instanceOf, work_uri))\n",
    "    records_bf.add((work_uri, BF.hasInstance, instance_uri))\n",
    "\n",
    "    # add an identifier bnode to the work using a function:\n",
    "    records_bf.add((work_uri, BF.identifiedBy, get_bf_identifier_dfk(dfk)))\n",
    "\n",
    "    # get field TI and add as title node:\n",
    "    records_bf.add((instance_uri, BF.title, get_bf_title(record)))\n",
    "\n",
    "    # get work language from LA\n",
    "    records_bf.add((work_uri, BF.language, get_work_language(record)))\n",
    "\n",
    "    # get TIUE field and add as translated title node:\n",
    "    # but only if the field exists!\n",
    "    if record.find(\"TIUE\") is not None and record.find(\"TIUE\").text != \"\":\n",
    "        records_bf.add((instance_uri, BF.title, get_bf_translated_title(record)))\n",
    "\n",
    "\n",
    "    # get toc, if it exists:\n",
    "    get_bf_toc(work_uri, record)\n",
    "    \n",
    "    # get and add main/original abstract:\n",
    "    # note: somehow not all records have one!\n",
    "    if record.find(\"ABH\") is not None:\n",
    "        records_bf.add((work_uri, BF.summary, get_bf_abstract(record)))\n",
    "\n",
    "    # get and add main/original abstract:\n",
    "    # note: somehow not all records have one!\n",
    "    if record.find(\"ABN\") is not None:\n",
    "        records_bf.add((work_uri, BF.summary, get_bf_secondary_abstract(record)))\n",
    "\n",
    "\n",
    "# print all the resulting triples:\n",
    "records_bf.serialize(\"ttl-data/bibframe_records.ttl\", format=\"turtle\")\n",
    "records_bf.serialize(\"ttl-data/bibframe_records.jsonld\", format=\"json-ld\")\n",
    "print(len(records_bf), \"triples\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uris and types for simplified profile (schema-org)\n",
    "\n",
    "For the simplified profile, we only need one entity per record (for now) and we give it the class schema:CreativeWork.\n",
    "Then we print the resulting triples into a separate file for the simplified profile that mostly uses schema.org properties and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684 triples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(len(root.findall(\"Record\")))\n",
    "\n",
    "for record in root.findall(\"Record\"):\n",
    "    # get the DFK identifier from the record:\n",
    "    dfk = record.find(\"DFK\").text\n",
    "\n",
    "    # create a URI for the work by attaching the dfk to the works namespace and \n",
    "    # then give it the correct schema.org class:\n",
    "    work_uri = WORKS[dfk]\n",
    "    records_schema.add((work_uri, RDF.type, SCHEMA.CreativeWork))\n",
    "\n",
    "    # get work language from LA\n",
    "    records_schema.add((work_uri, SCHEMA.inLanguage, get_work_language(record)))\n",
    "\n",
    "\n",
    "records_schema.serialize(\"ttl-data/schema_records.jsonld\", format=\"json-ld\")\n",
    "# records_schema.serialize(\"ttl-data/schema_records.ttl\", format=\"turtle\")\n",
    "print(len(records_schema), \"triples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.10.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star2BF - Star to Bibframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Literal\n",
    "from rdflib.namespace import RDF, RDFS, Namespace\n",
    "# from rdflib.namespace import SCHEMA, XSD\n",
    "from rdflib import BNode\n",
    "from rdflib import URIRef\n",
    "import xml.etree.ElementTree as ET\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an \"element tree\" from the records in my xml file so we can loop through them and do things with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = ET.parse(\"xml-data/records-440.xml\")\n",
    "# root = ET.parse(\"xml-data/records-322.xml\")\n",
    "# root = ET.parse(\"xml-data/records-395.xml\")\n",
    "# root = ET.parse(\"xml-data/records-214.xml\")\n",
    "root = ET.parse(\"/home/tina/Developement/psyndex-workflows/star-to-rdf/data/230424_000956/xml/records-556.xml\")\n",
    "\n",
    "# To see the source xml's structure, uncomment this function:\n",
    "# def print_element(element, depth=0):\n",
    "#     print(\"\\t\"*depth, element.tag, element.attrib, element.text)\n",
    "#     for child in element:\n",
    "#         print_element(child, depth+1)\n",
    "\n",
    "# for child in root.getroot()[:2]:\n",
    "#     print_element(child)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set a few namespace objects for bibframe, schema.org and for our resources (the works and instances) \n",
    "themselves.\n",
    "\n",
    "Then, we create two graphs from the xml source file, one to generate triples for our bibframe profile output, and the other for the simplified schema.org profile. \n",
    "\n",
    "Finally, we bind the prefixes with their appropriate namespaces to the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF = Namespace(\"http://id.loc.gov/ontologies/bibframe/\")\n",
    "BFLC = Namespace(\"http://id.loc.gov/ontologies/bflc/\")\n",
    "MADS = Namespace(\"http://www.loc.gov/mads/rdf/v1#\")\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "WORKS = Namespace(\"https://w3id.org/zpid/resources/works/\")\n",
    "INSTANCES = Namespace(\"https://w3id.org/zpid/resources/instances/\")\n",
    "PXC = Namespace(\"https://w3id.org/zpid/ontology/classes/\")\n",
    "PXP = Namespace(\"https://w3id.org/zpid/ontology/properties/\")\n",
    "LANG = Namespace (\"http://id.loc.gov/vocabulary/iso639-2/\")\n",
    "LOCID = Namespace(\"http://id.loc.gov/vocabulary/identifiers/\")\n",
    "ROLES = Namespace(\"https://w3id.org/zpid/vocabs/roles/\")\n",
    "\n",
    "# graph for bibframe profile:\n",
    "records_bf = Graph()\n",
    "\n",
    "kerndaten = Graph()\n",
    "kerndaten.parse(\"ttl-data/kerndaten.ttl\", format=\"turtle\")    \n",
    "# we need a new graph for the schema.org profile, so it won't just reuse the old triples from the other profile\n",
    "# records_schema = Graph()\n",
    "\n",
    "# Bind the namespaces to the prefixes we want to see in the output:\n",
    "records_bf.bind(\"bf\", BF) \n",
    "records_bf.bind(\"bflc\", BFLC) \n",
    "records_bf.bind(\"works\", WORKS)  \n",
    "# records_schema.bind(\"works\", WORKS) \n",
    "records_bf.bind(\"instances\", INSTANCES) \n",
    "records_bf.bind(\"pxc\", PXC) \n",
    "records_bf.bind(\"pxp\", PXP) \n",
    "records_bf.bind(\"lang\", LANG) \n",
    "records_bf.bind(\"schema\", SCHEMA) \n",
    "records_bf.bind(\"locid\", LOCID) \n",
    "records_bf.bind(\"mads\", MADS) \n",
    "records_bf.bind(\"roles\", ROLES) \n",
    "\n",
    "# records_schema.bind(\"instances\", INSTANCES) \n",
    "# todo: find out why the output uses \"schema1\" instead of \"schema\" for the schema.org namespace:\n",
    "# records_schema.bind(\"schema\", SCHEMA, override=True) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to do all the things\n",
    "\n",
    "We need functions for the different things we will do - to avoid one long monolith of a loop.\n",
    "\n",
    "This is where they will go. Examples: Create blank nodes for Idebtifiers, create nested contribution objects from disparate person entries in AUP, AUK, CS and COU fields, merge PAUP (psychauthor person names and ids) with the person's name in AUP...\n",
    "\n",
    "These functions will later be called at the bottom of this notebook, in a loop over all the xml records."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Function: Replace weird characters with unicode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "<>:5: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "/tmp/ipykernel_2009/892454608.py:5: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "  (\"&gt;\", \">\")\n"
     ]
    }
   ],
   "source": [
    "def replace_encodings(text):\n",
    "    cases = [\n",
    "        (\"&amp;\", \"&\"),\n",
    "        (\"&lt;\", \"<\"),\n",
    "        (\"&gt;\", \">\")\n",
    "        (\"^D$e\", \"€\"),\n",
    "        (\"^D#&gt;\", \"≥\"),\n",
    "        (\"^DEL\", \"…\"),\n",
    "        (\"^DIF\", \"∞\"),\n",
    "        (\"^D#=D\", \"≠\"),\n",
    "        (\"^DDS\", \"-\")\n",
    "        # lots more where this came from...\n",
    "    ]\n",
    "    for case in cases:\n",
    "        text = text.replace(case[0], case[1])   \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Adding DFK as an Identifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFK as id for Bibframe\n",
    "\n",
    "We want to add the DFK as a local bf:Identifier to the work (or instance?). \n",
    "We also want to say where the Identifier originates (to say it is from PSYNDEX/ZPID). \n",
    "\n",
    "The format for that is:\n",
    "```turtle\n",
    "<Work/Instance> bf:identifiedBy [\n",
    "    a bf:Local, pxc:DFK; \n",
    "    rdf:value \"1234456\"; \n",
    "    bf:source [\n",
    "        a bf:Source; bf:code \"ZPID.PSYNDEX.DFK\"\n",
    "    ]\n",
    "];\n",
    "```\n",
    "\n",
    "So, we need a blank node for the Identifier and inside, another nested bnode for the bf:Source. This is a function that will return such an identifier bnode to add to the work_uri. We are calling it way up down below in the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a function to be called in a for-loop while going through all records of the source xml, \n",
    "# which returns a new triple to add to the graph that has a bnode for the dfk identifier.\n",
    "# The predicate is \"bf:identifiedBy\" and the object is a blank node of rdf:Type \"bf:Identifier\" and \"bf:Local\":\n",
    "# The actual identifier is a literal with the text from the \"DFK\" element of the record.\n",
    "def get_bf_identifier_dfk(instance_uri, dfk):\n",
    "    # make a  BNODE of the Identifier class from the BF namespace:\n",
    "    # identifier = BNode()\n",
    "    identifier = URIRef(instance_uri + \"/identifier/dfk\")\n",
    "    identifier_source = BNode()\n",
    "    # records_bf.add ((identifier, RDF.type, BF.Identifier))\n",
    "    records_bf.add ((identifier, RDF.type, BF.Local))\n",
    "    records_bf.add ((identifier, RDF.type, PXC.DFK))\n",
    "    # build the source node:\n",
    "    records_bf.add((identifier_source, RDF.type, BF.Source))\n",
    "    records_bf.add((identifier_source, BF.code, Literal(\"ZPID.PSYNDEX.DFK\")))\n",
    "\n",
    "    # hang the id source node into the id node:\n",
    "    records_bf.add((identifier, BF.source, identifier_source))\n",
    "    records_bf.add((identifier, RDF.value, Literal(dfk)))\n",
    "    return (identifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Function: Replace languages with their language tag\n",
    "\n",
    "Can be used for different fields that are converted to langstrings or language uris. Use within other functions that work with the languages in different fields.\n",
    "\n",
    "Returns an array with two values: a two-letter langstring tag at [0] and a three-letter uri code for the library of congress language vocab at [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langtag_from_field(langfield):\n",
    "    # when passed a string from any language field in star, returns an array with two items. \n",
    "    # Index 0: two-letter langstring tag, e.g. \"de\"\n",
    "    # Index 1: two-letter iso langtag, e.g. \"ger\"\n",
    "    # can be used on these fields (it contains the different spellings found in them):\n",
    "    # \"LA\", \"LA2\", \"TIL\", \"TIUL\", \"ABLH\", \"ABLN\", \"TIUE |s\"\n",
    "    match langfield:\n",
    "        case \"german\" | \"de\" | \"GERM\" | \"Deutsch\" | \"GERMAN\" | \"GERMaN\" | \"German\" | \"Fi\":\n",
    "            return [\"de\", \"ger\"]\n",
    "        case \"en\" | \"ENGL\" | \"ENGLISH\" | \"Englisch\" | \"English\" | \"English; English\" | \"english\" :\n",
    "            return [\"en\", \"eng\"]\n",
    "        case \"BULG\" | \"Bulgarian\":\n",
    "            return [\"bg\", \"bul\"]\n",
    "        case \"SPAN\"| \"Spanish\":\n",
    "            return [\"es\", \"spa\"]\n",
    "        case \"Dutch\":\n",
    "            return [\"nl\", \"dut\"]\n",
    "        case \"CZEC\":\n",
    "            return [\"cs\", \"ces\"]\n",
    "        case \"FREN\" | \"French\":\n",
    "            return [\"fr\", \"fra\"]\n",
    "        case \"ITAL\" | \"Italian\":\n",
    "            return [\"it\", \"ita\"]\n",
    "        case \"PORT\" | \"Portuguese\":\n",
    "            return [\"pt\", \"por\"]\n",
    "        case \"JAPN\" | \"Japanese\":\n",
    "            return [\"jp\", \"jpn\"]\n",
    "        case \"HUNG\":\n",
    "            return [\"hu\", \"hun\"]\n",
    "        case \"RUSS\" | \"Russian\":\n",
    "            return [\"ru\", \"rus\"]\n",
    "        case \"NONE\" | \"Silent\":\n",
    "            return [\"zxx\", \"zxx\"]\n",
    "        case _:\n",
    "            return [\"und\", \"und\"] # for \"undetermined!\"\n",
    "\n",
    "# ---\n",
    "# these are also in those fields, but they are errors that should be repaired before migration!\n",
    "# X$English \n",
    "# EnglishX$\n",
    "# EnglishX$X$\n",
    "# $English\n",
    " \n",
    "# $German \n",
    "# GermanX$X$\n",
    "# X$$German\n",
    "# X$$GermanX$$German\n",
    "# GermanX$\n",
    "# GermanX$$EnglishX$$English\n",
    "# GermanX$$EnglishX$$EnglishX$$English\n",
    "# GermanX$English \n",
    "# GermanX$English X$English \n",
    "# GermanX$English X$English X$English \n",
    "# GermanX$English X$EnglishX$EnglishX$English\n",
    "# GermanX$English; English\n",
    "# GermanX$EnglishX$English\n",
    "# GermanX$EnglishX$EnglishX$English\n",
    "# GermanX$EnglishX$EnglishX$EnglishX$EnglishX$English\n",
    "# Fi (aus TIUL) - während TIL \"German\" ist! Das Dokument ist auch eindeutig Deutsch.\n",
    "\n",
    "# 4 (aus LA2) -> kann gelöscht werden, das Dokument ist in Deutsch und hat keine Zweitsprache!\n",
    "# Q (aus LA2) -> kann gelöscht werden, das Dokument ist in Deutsch und hat keine Zweitsprache! \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Get work language from LA\n",
    "\n",
    "Example\n",
    "\n",
    "```turtle\n",
    "@prefix lang: <http://id.loc.gov/vocabulary/iso639-2/> .\n",
    "<W> bf:language lang:ger .\n",
    "```\n",
    "\n",
    "Calls the generic language code lookup function above, get_langtag_from_field, passing the LA field content, returning a uri from the library of congress language vocabulary (built from namespace + 3-letter iso code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function \n",
    "def get_work_language(record):\n",
    "    work_language = get_langtag_from_field(record.find(\"LA\").text.strip())[1]\n",
    "    work_lang_uri = LANG[work_language]\n",
    "    return (work_lang_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create Instance Title nodes from fields TI, TIU, TIL, TIUE...\n",
    "\n",
    "Titles and Translated titles are attached to Instances. Translated titles also have a source, which can be DeepL, ZPID, or Original.\n",
    "\n",
    "Example:\n",
    "\n",
    "```turtle\n",
    "<Instance> bf:title \n",
    "        [a bf:Title; \n",
    "            bf:mainTitle \"Disentangling the process of epistemic change\"@en;\n",
    "            bf:subtitle \"The role of epistemic volition\"@en;\n",
    "        ],\n",
    "        [a pxc:TranslatedTitle;\n",
    "            rdfs:label \"Den Prozess des epistemischen Wandels entwirren: Die Rolle des epistemischen Willens.\"@de;\n",
    "            bf:mainTitle \"Den Prozess des epistemischen Wandels entwirren: Die Rolle des epistemischen Willens.\"@de;\n",
    "            bf:adminMetadata  [ \n",
    "                a bf:AdminMetadata ;\n",
    "                bflc:metadataLicensor  \"DeepL\";\n",
    "        ]\n",
    "        ].\n",
    "```\n",
    "\n",
    "- [x] add TI as bf:Title via bf:mainTitle\n",
    "- [x] add subtitle from TIU\n",
    "- [x] create a concatenated rdfs:label from TI and TIU\n",
    "- [x] add languages for maintitle and subtitle (from TIL and TIUL)\n",
    "\n",
    "- [x] add translated title from TIUE as pxc:TranslatedTitle with bf:mainTitle and rdfs:label \n",
    "- [x] add languages for translated title (from subfield TIU |s, or if unavailable, decide language based on TIL language: if de -> en and vice versa) \n",
    "- [x] find a way to create a source for the translated title (from \"(DeepL)\" at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a function to be called in a for-loop while going through all records of the source xml, \n",
    "# which returns a new triple to add to the graph that has a bnode for the dfk identifier.\n",
    "# The predicate is \"bf:identifiedBy\" and the object is a blank node of rdf:Type \"bf:Identifier\" and \"bf:Local\":\n",
    "# The actual identifier is a literal with the text from the \"DFK\" element of the record.\n",
    "def get_bf_title(instance_uri, record):\n",
    "    # make a  BNODE for the title:\n",
    "    # title = BNode()\n",
    "    title = URIRef(instance_uri + \"/title\")\n",
    "    # make it bf:Title class:\n",
    "    records_bf.add ((title, RDF.type, BF.Title))\n",
    "\n",
    "    # get the content of th TI field as the main title:\n",
    "    maintitle = record.find(\"TI\").text.strip()\n",
    "    # write a full title for the rdfs:label \n",
    "    # (update later if subtitle exists to add that)\n",
    "    fulltitle = maintitle\n",
    "    # set dafault language for main title:\n",
    "    maintitle_language = \"en\"\n",
    "    subtitle_language = \"en\"\n",
    "    # get language of main title - if exists!:\n",
    "    if record.find(\"TIL\") is not None:\n",
    "        maintitle_language = get_langtag_from_field(record.find(\"TIL\").text.strip())[0]\n",
    "        # if maintitle_language_til == \"German\":\n",
    "        #     maintitle_language = \"de\"\n",
    "        # else: just keep the default set above: \"en\"\n",
    "    # get language of subtitle:\n",
    "    if record.find(\"TIUL\") is not None:\n",
    "        subtitle_language = get_langtag_from_field(record.find(\"TIUL\").text.strip())[0]\n",
    "        # subtitle_language_tiul = record.find(\"TIUL\").text.strip()\n",
    "        # if subtitle_language_tiul == \"German\":\n",
    "        #     subtitle_language = \"de\"\n",
    "        # else: just keep the default set above: \"en\"\n",
    "\n",
    "    # add the content of TI etc via bf:mainTitle:\n",
    "    records_bf.add((title, BF.mainTitle, Literal(maintitle, lang=maintitle_language)))\n",
    "    # get content of the TIU field as the subtitle, \n",
    "    # _if_ it exists and has text in it:\n",
    "    if record.find(\"TIU\") is not None and record.find(\"TIU\") != \"\":\n",
    "        subtitle = record.find(\"TIU\").text.strip() # remove extraneous spaces\n",
    "        # concatenate a full title from main- and subtitle, \n",
    "        # separated with a : and overwrite fulltitle with that\n",
    "        fulltitle = fulltitle + \": \" + subtitle\n",
    "        # add the content of TIU to the bf:Title via bf:subtitle:\n",
    "        records_bf.add((title, BF.subtitle, Literal(subtitle, lang=subtitle_language)))\n",
    "\n",
    "    # add the concatenated full title to the bf:Title via rdfs:label:\n",
    "    # (we don't care if the main title's and subtitle's languages don't match - we just set the language of the main title as the full title's language)\n",
    "    records_bf.add((title, RDFS.label, Literal(fulltitle, lang=maintitle_language)))\n",
    "\n",
    "    # # hang the id source node into the id node:\n",
    "    # records_bf.add((identifier, BF.source, identifier_source))\n",
    "    return (title)\n",
    "\n",
    "# function for the translated title:\n",
    "def get_bf_translated_title(instance_uri, record):\n",
    "    # translated_title = BNode()\n",
    "    translated_title = URIRef(instance_uri + \"/title/translated\")\n",
    "    records_bf.add ((translated_title, RDF.type, PXC.TranslatedTitle))\n",
    "    fulltitle = record.find(\"TIUE\").text.strip()\n",
    "    fulltitle_language = \"de\"\n",
    "    # find a way to read subfield |s to get the actual language. \n",
    "    # it that doesn't exist, use the inverse of TIL!\n",
    "    # if fulltitle string ends with \"|s \" followed by some text (use a regex):\n",
    "    match = re.search(r'^(.*)\\s\\|s\\s(.*)', fulltitle)\n",
    "    if match:\n",
    "        fulltitle = match.group(1).strip()\n",
    "        fulltitle_language = get_langtag_from_field(match.group(2).strip())[0]\n",
    "    else:\n",
    "        # get the language in TIUE, if that field exists\n",
    "        if record.find(\"TIL\") is not None:\n",
    "            original_title_language_til = get_langtag_from_field(record.find(\"TIL\").text.strip())[0]\n",
    "            \n",
    "            # if it is German -> use inverse: \"en\"\n",
    "            if original_title_language_til == \"de\":\n",
    "                fulltitle_language = \"en\"\n",
    "            # else -> keep \"de\"\n",
    "\n",
    "    # check if the title contains a \"(DeepL)\" and cut it into a variable for the source:\n",
    "    titlesource = \"ZPID\" # translation source is \"ZPID\" by default\n",
    "    # note: we might be able to add source \"Original\" by finding out \n",
    "    # if the source of the secondary abstract is something other than ZPID!\n",
    "    match_source = re.search(r'^(.*)\\((DeepL)\\)$', fulltitle)\n",
    "    if match_source:\n",
    "        fulltitle = match_source.group(1).strip()\n",
    "        titlesource = match_source.group(2)\n",
    "\n",
    "    # build a source node for the translation:\n",
    "    titlesource_node = BNode ()\n",
    "    records_bf.add ((titlesource_node, RDF.type, BF.AdminMetadata))\n",
    "    records_bf.add ((titlesource_node, BFLC.metadataLicensor, Literal(titlesource)))\n",
    "\n",
    "    # add the title string to the bnode:\n",
    "    records_bf.add((translated_title, BF.mainTitle, Literal(fulltitle, lang=fulltitle_language)))\n",
    "    records_bf.add((translated_title, RDFS.label, Literal(fulltitle, lang=fulltitle_language)))\n",
    "    records_bf.add((translated_title, BF.adminMetadata, titlesource_node))\n",
    "\n",
    "    return (translated_title)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Add Abstracts - original abstract (from fields ABH, ABLH, ABSH1, ABSH2) and translated/secondary abstract (from ABN, ABLN, ASN1, ASN2)\n",
    "\n",
    "- Main Abstract: \n",
    "    - abstract text is in field ABH.\n",
    "    - abstract language is in ABLH (\"German\" or \"English\")\n",
    "    - abstract original source is in ASH1 (\"Original\" or \"ZPID\")\n",
    "    - agent who edited the original, if that happened, is in ASH2 ()\n",
    "- Secondary Abstract \n",
    "    - abstract text is in field ABN.\n",
    "    - abstract language is in ABLN (\"German\" or \"English\")\n",
    "    - abstract original source is in ASN1 (\"Original\" or \"ZPID\")\n",
    "    - agent who edited the original, if that happened, is in ASN2 ()\n",
    "\n",
    "Scheme:\n",
    "\n",
    "```turtle\n",
    "<W> bf:summary \n",
    "    [ a pxc:Abstract , bf:Summary ;\n",
    "        rdfs:label  \"Background: Loneliness is ...\"@en ;\n",
    "        bf:adminMetadata  [ \n",
    "            a bf:AdminMetadata ;\n",
    "            bflc:metadataLicensor  \"Original\";\n",
    "            bf:descriptionModifier \"ZPID\"\n",
    "        ]\n",
    "] .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the original abstract:\n",
    "def get_bf_abstract(work_uri, record):\n",
    "    # abstract = BNode()\n",
    "    abstract = URIRef(work_uri + \"/abstract\")\n",
    "    records_bf.add ((abstract, RDF.type, PXC.Abstract))\n",
    "    # get abstract text from ABH\n",
    "    abstracttext = record.find(\"ABH\").text.strip()\n",
    "    # get abstract language from ABLH (\"German\" or \"English\")\n",
    "    abstract_language = \"en\" # set default\n",
    "    if record.find(\"ABLH\") is not None:\n",
    "        abstract_language = get_langtag_from_field(record.find(\"ABLH\").text.strip())[0]\n",
    "\n",
    "    # add the text to the bnode:\n",
    "    records_bf.add ((abstract, RDFS.label, Literal(abstracttext, lang=abstract_language)))\n",
    "\n",
    "    # get abstract original source from ASH1 (\"Original\" or \"ZPID\")\n",
    "    abstract_source = \"Original\" # default\n",
    "    # create a blank node for admin metadata:\n",
    "    abstract_source_node = BNode()\n",
    "    records_bf.add((abstract_source_node, RDF.type, BF.AdminMetadata))\n",
    "\n",
    "    if record.find(\"ASH1\") is not None:\n",
    "        # overwrite default (\"Original\") with what we find in ASH1:\n",
    "        abstract_source = record.find(\"ASH1\").text.strip()\n",
    "    \n",
    "    # write final source text into source node:\n",
    "    records_bf.add((abstract_source_node, BFLC.metadataLicensor, Literal(abstract_source)))\n",
    "\n",
    "    # here is a list of known zpid employee tags, we will use them later to replace these with \"ZPID\" if found in ASH2:\n",
    "\n",
    "    # and this is a list of things we want to replace with \"Original\":\n",
    "    \n",
    "\n",
    "    # get optional agent who edited the original abstract from ASH2\n",
    "    if record.find(\"ASH2\") is not None:\n",
    "        # note what we find in ABSH2:\n",
    "        abstract_editor = record.find(\"ASH2\").text.strip()\n",
    "        # todo: replace known zpid person initials with \"ZPID\"\n",
    "        # \"Juergen Wiesenhuetter\",\n",
    "        # \"Joachim H. Becker\",\"Udo Wolff\", \"Juergen Beling\", \n",
    "        # \"Joachim H. Mueller\", \"Angelika Zimmer\", \"Annelie Wiertz\", \"Beate Minsel\", \"Berndt Zuschlag\",  \"Doris Lecheler\", \"Elke Bone\", \"Guenter Krampen\", \"Hella Lenders\", \"Jutta Rohlmann\", \"Juergen Howe\", \"Manfred Opitz\", \"Manfred Fischer\", \"Paul Klein\", \"Sigrun-Heide Filipp\", \"Thomas W. Franke\", \"Ulrike Fischer\", \"Yrla M. Labouvie\", \n",
    "        # \"K.Si\", \"L.F.T.\", \"M.G.\", \"I.D.\" , \"A.Bi.\", \"A.G.\", \"A.C.\", \"U.R.W\", \"U\", \"C.Si\", \"pe.k\", \"r\", \"R.N\", \"Ve.K.\",   \n",
    "\n",
    "        # if \"Author\" or \"Autor\" -> \"Original\"\n",
    "        # and what if \"DeepL\"???\n",
    "        # or \"FIS Bildung\", \"GESIS Fachinformation für die Sozialwissenschaften, Bonn\", \"Kriminologische Zentralstelle\", \n",
    "        # and add it via decription modifier:\n",
    "        records_bf.add((abstract_source_node, BF.descriptionModifier, Literal(abstract_editor)))\n",
    "\n",
    "\n",
    "    #add the source node to the abstract node:\n",
    "    records_bf.add((abstract, BF.adminMetadata, abstract_source_node))\n",
    "    # and return the completed node:\n",
    "    return (abstract)\n",
    "\n",
    "def get_bf_secondary_abstract(work_uri, record):\n",
    "    # abstract = BNode()\n",
    "    abstract = URIRef(work_uri + \"/abstract/secondary\")\n",
    "    records_bf.add ((abstract, RDF.type, PXC.Abstract))\n",
    "    records_bf.add ((abstract, RDF.type, PXC.SecondaryAbstract))\n",
    "    abstracttext = record.find(\"ABN\").text.strip()\n",
    "    \n",
    "    abstract_language = \"de\" # fallback default\n",
    "    if record.find(\"ABLN\") is not None:\n",
    "        abstract_language = get_langtag_from_field(record.find(\"ABLN\").text.strip())[0]\n",
    "    \n",
    "    records_bf.add ((abstract, RDFS.label, Literal(abstracttext, lang=abstract_language)))\n",
    "    \n",
    "    abstract_source_node = BNode()\n",
    "    records_bf.add((abstract_source_node, RDF.type, BF.AdminMetadata))\n",
    "    abstract_source = \"Original\" # fallback default\n",
    "    if record.find(\"ASN1\") is not None:\n",
    "        # overwrite default (\"Original\") with what we find in ASH1:\n",
    "        abstract_source = record.find(\"ASN1\").text.strip()\n",
    "    \n",
    "    records_bf.add((abstract_source_node, BFLC.metadataLicensor, Literal(abstract_source)))\n",
    "\n",
    "    # get optional agent who edited the original abstract from ASH2\n",
    "    if record.find(\"ASN2\") is not None:\n",
    "        # note what we find in ABSN2:\n",
    "        abstract_editor = record.find(\"ASN2\").text.strip()\n",
    "        # and add it via decription modifier:\n",
    "        records_bf.add((abstract_source_node, BF.descriptionModifier, Literal(abstract_editor)))\n",
    "\n",
    "    #add the source node to the abstract node:\n",
    "    records_bf.add((abstract, BF.adminMetadata, abstract_source_node))\n",
    "    # and return the completed node:\n",
    "    return (abstract)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Function to split Table of Content from the Abstract field (ABH)\n",
    "\n",
    "This usually starts with \" - Inhalt: \" (for German Abstracts) or \" - Contents: \" (in English abstracts) and ends at the end of the field.\n",
    "It can contain a numbered list of chapters or sections as a long string. It can also contain a uri from dnb namespace instead or in addition!\n",
    "\n",
    "Examples:\n",
    "- \" - Contents: (1) ...\"\n",
    "- \" - Inhalt: https://d-nb.info/1256712809/04</ABH>\" (URI pattern: \"https://d-nb.info/\" + \"1256712809\" 10 digits + \"/04\")\n",
    "\n",
    "Example:\n",
    "\n",
    "```turtle\n",
    "<W> bf:tableOfContents [\n",
    "    a bf:TableOfContents;\n",
    "    rdfs:label \"(1) Wünsche, J., Weidmann, R. &amp; Grob, A. (n. d.). Happy in the same way? The link between domain satisfaction and overall life satisfaction in romantic couples. Manuscript submitted for publication. (2) Wünsche, J., Weidmann,...\";\n",
    "] .\n",
    "```\n",
    "\n",
    "Or\n",
    "\n",
    "```turtle\n",
    "<W> bf:tableOfContents [\n",
    "    a bf:TableOfContents;\n",
    "    rdf:value \"https://d-nb.info/1002790794/04\"^^xsd:anyURI ;\n",
    "] .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bf_toc(work_uri, record):\n",
    "    # read the abstract in ABH\n",
    "    contents = \"\"\n",
    "    if record.find(\"ABH\") is not None:\n",
    "        abstracttext = record.find(\"ABH\").text.strip()\n",
    "        # check via regex if there is a \" - Inhalt: \" or \" - Contents: \" in it.\n",
    "        # if so, split out what comes after. Drop the contents/inhalt part itself.\n",
    "        match = re.search(r'^(.*)[-–]\\s*(?:Contents|Inhalt)\\s*:\\s*(.*)$', abstracttext)\n",
    "        if match:\n",
    "            abstracttext = match.group(1).strip()\n",
    "            contents = match.group(2).strip()\n",
    "\n",
    "    # also check if what comes is either a string or a uri following thegiven pattern\n",
    "    # and export one as a rdfs_label and the other as rdf:value \"...\"^^xsd:anyUrl (remember to add XSD namespace!)\n",
    "    # also remember that we should only create a node and attach it to the work\n",
    "    # if a) ABH exists at all and\n",
    "    # b) the regex is satisfied.\n",
    "    # So I guess we must do the whole checking and adding procedure in this function!\n",
    "\n",
    "    # only return an added triple if the toc exisits, otherwise return nothing:\n",
    "    if contents:\n",
    "        return records_bf.add((work_uri, BF.tableOfContents, Literal(contents)))\n",
    "    else: \n",
    "        return None\n",
    "    # return records_bf.add((work_uri, BF.tableOfContents, Literal(\"test\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create Person Contribution nodes from Fields AUP, EMID, EMAIL, AUK, PAUP, CS and COU\n",
    "\n",
    "Use this scheme:\n",
    "\n",
    "```turtle\n",
    "<Work> a bf:Work;\n",
    "    bf:contribution \n",
    "    [\n",
    "        # the Bibframe Contribution includes, as usual, an agent and their role,\n",
    "        # but is supplemented with an Affiliation (in the context of that work/while it was written),\n",
    "        # and a position in the author sequence.\n",
    "        a bf:Contribution, bflc:PrimaryContribution; \n",
    "        bf:agent \n",
    "        [\n",
    "            a bf:Person, schema:Person; \n",
    "            rdfs:label \"Trillitzsch, Tina\"; # name when creating work\n",
    "            schema:givenName \"Tina\"; schema:familyName \"Trillitzsch\";\n",
    "            owl:sameAs <https://w3id.org/zpid/person/tt_0000001>, <https://orcid.org/0000-0001-7239-4844>; # authority uris of person (local, orcid)\n",
    "            bf:identifiedBy [a bf:Local, pxc:PsychAuthorsID; rdf:value \"p01979TTR\"; #legacy authority ID\n",
    "            ];\n",
    "            bf:identifiedBy [a bf:Identifier, locid:orcid; rdf:value \"0000-0001-7239-4844\"; # ORCID \n",
    "            ];\n",
    "        ]\n",
    "        # we use a model inspired by Option C in Osma Suominen'a suggestion for https://github.com/dcmi/dc-srap/issues/3\n",
    "        # adding the Affiliation into the Contribution, separate from the agent itself, since the affiliation\n",
    "        # is described in the context of this work, not not as a statement about the person's\n",
    "        # current affiliation:\n",
    "        mads:hasAffiliation [\n",
    "            a mads:Affiliation;\n",
    "            # Affiliation blank node has info about the affiliation org (including persistent identifiers),\n",
    "            # the address (country with geonames identifier),\n",
    "            # and the person's email while affiliated there.\n",
    "            mads:organization [\n",
    "                a bf:Organization; \n",
    "                rdfs:label \"Leibniz Institute of Psychology (ZPID); Digital Research Development Services\"; # org name when work was created\n",
    "                owl:sameAs <https://w3id.org/zpid/org/zpid_0000001>, <https://ror.org/0165gz615>; # authority uris of org (local, ror)\n",
    "                # internal id and ror id as literal identifiers:\n",
    "                bf:identifiedBy [a bf:Local, pxc:ZpidCorporateBodyId; rdf:value \"0000001\"; ];\n",
    "                bf:identifiedBy [a bf:Identifier; locid:ror; rdf:value \"0165gz615\"; ];\n",
    "            ];\n",
    "            mads:hasAffiliationAddress [a mads:Address;\n",
    "                mads:country [\n",
    "                    a mads:Country, bf:Place;\n",
    "                    rdfs:label \"Germany\";\n",
    "                    bf:identifiedBy [a bf:Identifier, locid:geonames; rdf:value \"2921044\"; ];\n",
    "                    owl:sameAs <https://w3id.org/zpid/place/country/ger>;\n",
    "                ]\n",
    "            ];\n",
    "            mads:email <mailto:ttr@leibniz-psychology.org>; # correspondence author email\n",
    "        ];\n",
    "        bf:role <http://id.loc.gov/vocabulary/relators/aut>;\n",
    "        pxp:contributionPosition 1; bf:qualifier \"first\"; # first author in sequence: our own subproperty of bf:qualifier & schema:position (also: middle, last)\n",
    "    ].\n",
    "```\n",
    "\n",
    "Todos:\n",
    "- [x] create blank node for contribution and add agent of type bf:Person\n",
    "- [x] add author position (first, middle, last plus order number) to the contribution\n",
    "- [x] make first author a bflc:PrimaryContribution\n",
    "- [x] match AUP with PAUP to get person names and ids (normalize first)\n",
    "- [x] extend AUP-PAUP match with lookup in kerndaten table/ttl to compare schema:alternatename of person with name in AUP (but first before normalization)\n",
    "- [x] add ORCID to the person's blank node (doesn't add 4 ORCIDs for unknown reason - maybe duplicates?)\n",
    "- [x] add EMAIL to person's blank node (either to person in EMID or to first author)\n",
    "- [x] add affiliation from CS field and COU field to first author\n",
    "- [x] add Affiliation blank node with org name, country to each author that has these subfields in their AUP (|i and |c)\n",
    "- [x] add role from AUP subfield |f\n",
    "- [x] add country geonames id using lookup table\n",
    "- [ ] move mads:email Literal from bf:Contribution to mads:Affiliation\n",
    "- [ ] later: reconcile affiliations to add org id, org ror id (once we actually have institution authority files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import c\n",
    "\n",
    "\n",
    "def add_bf_contributor_person_role(role):\n",
    "    # return role_uri\n",
    "    return URIRef(ROLES + role)\n",
    "\n",
    "def country_geonames_lookup(country):\n",
    "    cases = [\n",
    "        (\"Germany\", \"2921044\"),\n",
    "        (\"United States\", \"6252001\"),\n",
    "        (\"United Kingdom\", \"2635167\"),\n",
    "        (\"Austria\", \"2782113\"),\n",
    "        (\"Switzerland\", \"2658434\"),\n",
    "        (\"Netherlands\", \"2750405\"),\n",
    "        (\"Belgium\", \"2802361\"),\n",
    "        (\"France\", \"3017382\"),\n",
    "        (\"Italy\", \"3175395\"),\n",
    "        (\"Spain\", \"2510769\"),\n",
    "        (\"Japan\", \"1861060\"),\n",
    "        (\"Bulgaria\", \"732800\"),\n",
    "        (\"Hungary\", \"719819\"),\n",
    "        (\"Czech Republic\", \"3077311\"),\n",
    "        (\"Portugal\", \"2264397\"),\n",
    "        (\"Russia\", \"2017370\"),\n",
    "        (\"Poland\", \"798544\"),\n",
    "        (\"Greece\", \"390903\"),\n",
    "        (\"Sweden\", \"2661886\"),\n",
    "        (\"Denmark\", \"2623032\"),\n",
    "        (\"Luxembourg\", \"2960313\"),\n",
    "        (\"Taiwan\", \"1668284\"),\n",
    "        (\"Norway\", \"3144096\"),\n",
    "        (\"Finland\", \"660013\"),\n",
    "        (\"Ireland\", \"2963597\"),\n",
    "        (\"Canada\", \"6251999\"),\n",
    "        (\"Australia\", \"2077456\"),\n",
    "        (\"New Zealand\", \"2186224\"),\n",
    "        (\"South Africa\", \"953987\"),\n",
    "        (\"People's Republic of China\", \"1814991\"),\n",
    "        (\"Turkey\", \"298795\"),\n",
    "        (\"Brazil\", \"3469034\"),\n",
    "        (\"Cuba\", \"3562981\"),\n",
    "        (\"Georgia\", \"614540\"),\n",
    "        (\"Iran\", \"130758\"),\n",
    "        (\"India\", \"1269750\"),\n",
    "    ]\n",
    "    for case in cases:\n",
    "        if case[0].casefold() == str(country).casefold():\n",
    "            return case[1]\n",
    "    return None\n",
    "\n",
    "def normalize_names(familyname,givenname):\n",
    "    familyname_normalized = familyname.replace(\"ä\", \"ae\").replace(\"ö\", \"oe\").replace(\"ü\", \"ue\").replace(\"Ä\", \"Ae\").replace(\"Ö\", \"Oe\").replace(\"Ü\", \"Ue\").replace(\"ß\", \"ss\")\n",
    "    # generate an abbreviated version of givenname (only the first letter), but \n",
    "    if givenname:\n",
    "        givenname_abbreviated = givenname[0] + \".\"\n",
    "        # generate a normalized version of the name by concatenating the two with a comma as the separator:\n",
    "        fullname_normalized = familyname_normalized + \", \" + givenname_abbreviated\n",
    "    return fullname_normalized\n",
    "\n",
    "def match_paup(record, person_node, personname_normalized):\n",
    "    # loop through all PAUPs and check if the name matches the normalized personname\n",
    "    for paup in record.findall(\"PAUP\"):\n",
    "        # given a string such as \"Forkmann, Thomas |n p06946TF |u https://www.psychauthors.de/psychauthors/index.php?wahl=forschung&amp;#38;uwahl=psychauthors&amp;#38;uuwahl=p06946TF\"\n",
    "        # split into family name, given name and paId where \"Forkmann\" is the family name, \"Thomas\" is the given name and \"p06946TF\" is the paId:\n",
    "        paup_split = paup.text.strip().split(\"|n\")[0].strip().split(\",\")\n",
    "        if len(paup_split) > 1:\n",
    "            paup_familyname = paup_split[0].strip()\n",
    "            paup_givenname = paup_split[1].strip()\n",
    "            paId = paup.text.strip().split(\"|n\")[1].strip().split(\"|\")[0].strip()\n",
    "            \n",
    "            # generate a normalized version of paup_familyname:\n",
    "            paup_name_normalized = normalize_names(paup_familyname,paup_givenname)\n",
    "            # generate a uri for the person from the paId that can match the one in kerndaten.ttl generated from psychauthors database:\n",
    "            person_uri = URIRef(\"https://w3id.org/zpid/person/\" + paId)\n",
    "            \n",
    "            # now check if the normalized name from PAUP matches the normalized person name from AUP:\n",
    "            # if they match and there is a matching person in the kerndaten.ttl graph, add the person uri as schema:sameAs and the current preferred name from psychauthors as schema:preferredName, then return the paId:\n",
    "            if paup_name_normalized == personname_normalized and (person_uri, RDF.type, SCHEMA.Person) in kerndaten:\n",
    "                # for debugging, print the actual name in the matching PAUP:\n",
    "                #records_bf.add((person_node, PXP.paupName, Literal(paup_familyname + \", \" + paup_givenname)))\n",
    "                records_bf.add((person_node, SCHEMA.sameAs, person_uri))\n",
    "                # add the preferred name from kerndaten as schema:preferredName:\n",
    "                records_bf.add((person_node, SCHEMA.preferredName, kerndaten.value(person_uri, SCHEMA.name)))\n",
    "                # return the psychauthors ID:\n",
    "                return paId\n",
    "            # but if PAUP and AUP names are no match, even normalized,\n",
    "            # go through all the alternate names in kerndaten for that Psychauthors ID and check if they match the normalized person name from AUP (this will even find completely changed names, from maiden name to married name etc.):\n",
    "            elif paup_name_normalized != personname_normalized and (person_uri, RDF.type, SCHEMA.Person) in kerndaten:\n",
    "                for alternatename in kerndaten.objects(person_uri, SCHEMA.alternateName):\n",
    "                    # split alternatename into first and last name:\n",
    "                    alternatename_split = alternatename.split(\",\")\n",
    "                    if len(alternatename_split) > 1:\n",
    "                        alternatename_familyname = alternatename_split[0].strip()\n",
    "                        alternatename_givenname = alternatename_split[1].strip()\n",
    "                        # generate a normalized version of alternatename_familyname to compare with PAUP name later:\n",
    "                        alternatename_normalized = normalize_names(alternatename_familyname,alternatename_givenname)\n",
    "                        if personname_normalized == alternatename_normalized:\n",
    "                            # we have found another match!\n",
    "                            # add the uri as schema:sameAs and put the current preferred name from psychauthors here, too (for debugging purposes):\n",
    "                            records_bf.add((person_node, SCHEMA.sameAs, person_uri))\n",
    "                            records_bf.add((person_node, SCHEMA.preferredName, kerndaten.value(person_uri, SCHEMA.name)))\n",
    "                            return paId\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "def get_orcid(record, person_node, personname):\n",
    "    # loop through all ORCIDs and check if the name matches the personname\n",
    "    for orcid in record.findall(\"ORCID\"):\n",
    "        # go through all ORCID fields and check for matches of personname with the text before \"|u\": \n",
    "        # split the orcid string into the orcid id and the name:\n",
    "        orcid_split = orcid.text.strip().split(\"|u\")\n",
    "        \n",
    "        # if there is a name part, compare it to the personname:\n",
    "        if len(orcid_split) > 1:\n",
    "            orcid_name = orcid_split[0].strip()\n",
    "            orcidId = orcid_split[1].strip()\n",
    "            # clean up the orcid_id by removing spaces that sometimes sneak in when entering them in the database:\n",
    "            orcidId = orcidId.replace(\" \", \"\")\n",
    "            \n",
    "            # by the way, here is a regex pattern for valid orcids:\n",
    "            # orcid_pattern = re.compile(r'^\\d{4}-\\d{4}-\\d{4}-\\d{3}[0-9X]$')\n",
    "            # and a way, check if the orcid id matches the pattern:\n",
    "            # if not orcid_pattern.match(orcidId):\n",
    "            #     print(\"invalid orcid: \" + orcidId)\n",
    "\n",
    "            # if the name matches, return the orcid id for adding it to the person node:\n",
    "            if orcid_name == personname:\n",
    "                return orcidId\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "def build_affiliation_nodes(person_affiliation, person_affiliation_country):\n",
    "    # is passed two string: the affiliation name and the affiliation country name\n",
    "    # make a blank node for the affiliation and make it class mads:Affiliation:\n",
    "    person_affiliation_node = BNode()\n",
    "    records_bf.add((person_affiliation_node, RDF.type, MADS.Affiliation))\n",
    "    # make a blank node for the affiliation organization and make it class bf:Organization:\n",
    "    person_affiliation_org_node = BNode()\n",
    "    records_bf.add((person_affiliation_org_node, RDF.type, BF.Organization))\n",
    "    # add the affiliation organization node to the affiliation node:\n",
    "    records_bf.add((person_affiliation_node, MADS.organization, person_affiliation_org_node))\n",
    "    # add the affiliation string to the affiliation org node:\n",
    "    records_bf.add((person_affiliation_org_node, RDFS.label, Literal(person_affiliation)))\n",
    "\n",
    "    # make a blank node for the affiliation address and make it class mads:Address:\n",
    "    person_affiliation_address_node = BNode()\n",
    "    records_bf.add((person_affiliation_address_node, RDF.type, MADS.Address))\n",
    "    # add a country node to the affiliation address node:\n",
    "    person_affiliation_country_node = BNode()\n",
    "    records_bf.add((person_affiliation_country_node, RDF.type, MADS.Country))\n",
    "    # add the country node to the affiliation address node:\n",
    "    records_bf.add((person_affiliation_address_node, MADS.country, person_affiliation_country_node))\n",
    "    # add the affiliation address string to the affiliation address node:\n",
    "    records_bf.add((person_affiliation_country_node, RDFS.label, Literal(person_affiliation_country)))\n",
    "\n",
    "    # if the country is in the geonames lookup table, add the geonames uri as sameAs and the geonames id as an identifier:\n",
    "    if country_geonames_lookup(person_affiliation_country):\n",
    "        geonamesId = country_geonames_lookup(person_affiliation_country)\n",
    "        geonames_uri = URIRef(\"http://geonames.org/\" + geonamesId + \"/\")\n",
    "        records_bf.add((person_affiliation_country_node, SCHEMA.sameAs, geonames_uri))\n",
    "        # add the geonames identifier:\n",
    "        person_affiliation_country_identifier_node = BNode()\n",
    "        records_bf.add((person_affiliation_country_identifier_node, RDF.type, BF.Identifier))\n",
    "        records_bf.add((person_affiliation_country_identifier_node, RDF.type, LOCID.geonames))\n",
    "        records_bf.add((person_affiliation_country_identifier_node, RDF.value, Literal(geonamesId)))\n",
    "        records_bf.add((person_affiliation_country_node, BF.identifier, person_affiliation_country_identifier_node))\n",
    "    # add the affiliation address node to the affiliation node:\n",
    "    records_bf.add((person_affiliation_node, MADS.hasAffiliationAddress, person_affiliation_address_node))\n",
    "\n",
    "    # return the finished affiliation node with all its children and attached strings:\n",
    "    return person_affiliation_node\n",
    "\n",
    "# the full function that creates a contribution node for each person in AUP:\n",
    "# first, get all AUPs in a record and create a blank node for each of them\n",
    "def add_bf_contributor_person(work_uri, record):\n",
    "    # initialize a counter for the contribution position and a variable for the contribution qualifier:\n",
    "    contribution_counter = 0\n",
    "    contribution_qualifier = None\n",
    "    \n",
    "    for person in record.findall(\"AUP\"):\n",
    "        # count how often we've gone through the loop to see the author position:\n",
    "        contribution_counter += 1\n",
    "        # make a blank node for the bf:Contribution:\n",
    "        # contribution_node = BNode()\n",
    "        contribution_node = URIRef(work_uri + \"/contribution/\" + str(contribution_counter))\n",
    "        records_bf.add((contribution_node, RDF.type, BF.Contribution))\n",
    "        \n",
    "        # make a blank node for the person:\n",
    "        person_node = BNode()\n",
    "        records_bf.add((person_node, RDF.type, BF.Person))\n",
    "\n",
    "        \n",
    "\n",
    "        # add the counter as an author position to the contribution node:\n",
    "        records_bf.add((contribution_node, PXP.contributionPosition, Literal(contribution_counter)))\n",
    "\n",
    "        # if we are in the first loop, set \"contrution_qualifier\" to \"first\":\n",
    "        if contribution_counter == 1:\n",
    "            contribution_qualifier = \"first\"\n",
    "            records_bf.add((contribution_node, RDF.type, BFLC.PrimaryContribution))\n",
    "        # if we are in the last loop, set \"contribution_qualifier\" to \"last\":\n",
    "        elif contribution_counter == len(record.findall(\"AUP\")):\n",
    "            contribution_qualifier = \"last\"\n",
    "        # if we are in any other loop but the first or last, set \"contribution_qualifier\" to \"middle\":\n",
    "        else:\n",
    "            contribution_qualifier = \"middle\"\n",
    "\n",
    "        # add the contribution qualifier to the contribution node:\n",
    "        records_bf.add((contribution_node, BF.qualifier, Literal(contribution_qualifier)))\n",
    "\n",
    "        # add the name from AUP to the person node, but only use the text before the first |:\n",
    "        personname = person.text.strip().split(\"|\")[0].strip()\n",
    "        records_bf.add((person_node, RDFS.label, Literal(personname)))        \n",
    "\n",
    "        # initialize variables for later use:\n",
    "        personname_normalized = None\n",
    "        orcidId = None\n",
    "\n",
    "        # split personname into first and last name:\n",
    "        personname_split = personname.split(\",\")\n",
    "        if len(personname_split) > 1:\n",
    "            familyname = personname_split[0].strip()\n",
    "            givenname = personname_split[1].strip()\n",
    "            records_bf.add((person_node, SCHEMA.familyName, Literal(familyname)))\n",
    "            records_bf.add((person_node, SCHEMA.givenName, Literal(givenname)))\n",
    "            # generate a normalized version of familyname to compare with PAUP name later:\n",
    "            personname_normalized = normalize_names(familyname,givenname)\n",
    "            # for debugging, print the normalized name:\n",
    "            # records_bf.add((person_node, PXP.normalizedName, Literal(personname_normalized)))\n",
    "\n",
    "        # call the function match_paup to match the personname from AUP with the PAUPs:\n",
    "        paId = match_paup(record, person_node, personname_normalized)\n",
    "        if paId is not None:\n",
    "            # create a blank node for the identifier:\n",
    "            # we coulkd do this into the function, but then I will have to return something else\n",
    "            psychauthors_identifier_node = BNode()\n",
    "            records_bf.add((psychauthors_identifier_node, RDF.type, BF.Identifier))\n",
    "            records_bf.add((psychauthors_identifier_node, RDF.type, BF.Local))\n",
    "            records_bf.add((psychauthors_identifier_node, RDF.type, PXC.PsychAuthorsID))\n",
    "            records_bf.add((psychauthors_identifier_node, RDF.value, Literal(paId)))\n",
    "            # add the identifier node to the person node:\n",
    "            records_bf.add((person_node, BF.identifiedBy, psychauthors_identifier_node))\n",
    "            # create a urL from the paid and add it as a \"webpage describing this entity\" to the person node:\n",
    "            psychauthors_url = \"https://www.psychauthors.de/psychauthors/index.php?wahl=forschung&uwahl=psychauthors&uuwahl=\" + paId\n",
    "            records_bf.add((person_node, SCHEMA.mainEntityOfPage, URIRef(psychauthors_url)))\n",
    "        \n",
    "        # call the function get_orcid to match the personname with the ORCIDs in the record:\n",
    "        orcidId = get_orcid(record, person_node, personname)\n",
    "        if orcidId is not None:\n",
    "            # create a blank node for the identifier:\n",
    "            orcid_identifier_node = BNode()\n",
    "            records_bf.add((orcid_identifier_node, RDF.type, BF.Identifier))\n",
    "            records_bf.add((orcid_identifier_node, RDF.type, LOCID.orcid))\n",
    "            records_bf.add((orcid_identifier_node, RDF.value, Literal(orcidId)))\n",
    "            # add the identifier node to the person node:\n",
    "            records_bf.add((person_node, BF.identifiedBy, orcid_identifier_node))\n",
    "            # add the orcid id as a sameAs link to the person node:\n",
    "            orcid_uri = \"https://orcid.org/\" + orcidId\n",
    "            records_bf.add((person_node, SCHEMA.sameAs, URIRef(orcid_uri)))\n",
    "\n",
    "        ## ----- \n",
    "        # Getting Affiliations and their countries from first, CS and COU (only for first author), and then from subfields |i and |c in AUP (for newer records)\n",
    "        ## -----\n",
    "        \n",
    "        # initialize variables we'll need for adding affiliations and country names from AUP |i and CS/COU/ADR:\n",
    "        affiliationstring = None\n",
    "        affiliation_country = None\n",
    "      \n",
    "        # match affiliations in CS and COU to first contribution/author:\n",
    "        # dont add ADR here yet (even if this is the place for it - we may drop that info anyway.\n",
    "        # look for the field CS:\n",
    "        # if the contribution_counter is 1 (i.e. if this is the first loop/first author), add the affiliation to the person node:\n",
    "        if contribution_counter == 1:\n",
    "            if record.find(\"CS\") is not None:\n",
    "                # get the content of the CS field:\n",
    "                affiliationstring = record.find(\"CS\").text.strip()\n",
    "\n",
    "            if record.find(\"COU\") is not None:\n",
    "                # get the country from the COU field:\n",
    "                affiliation_country = record.find(\"COU\").text.strip()\n",
    "\n",
    "                \n",
    "        ## Get affiliation from AUP |i, country from |c:\n",
    "        # no looping necessary here, just check if a string |i exists in AUP and if so, add it to the person node:\n",
    "        # if AUP contains \"|i \", use anything after it and before the end of the string or before another \"|\" as the affiliation string:\n",
    "        if person.text.strip().find(\"|i \") > -1:\n",
    "            # save that text in a variable:\n",
    "            affiliationstring = person.text.strip().split(\"|i\")[1].strip().split(\"|\")[0].strip()\n",
    "            \n",
    "        # now check if there is a country in |c:\n",
    "        if person.text.strip().find(\"|c \") > -1:\n",
    "            # save that text in a variable:\n",
    "            affiliation_country = person.text.strip().split(\"|c\")[1].strip().split(\"|\")[0].strip()\n",
    "\n",
    "        # pass this to function build_affiliation_nodes to get a finished affiliation node:\n",
    "        if affiliationstring is not None:\n",
    "            affiliation_node = build_affiliation_nodes(affiliationstring, affiliation_country)\n",
    "            # add the affiliation node to the contribution node:\n",
    "            records_bf.add((contribution_node, MADS.hasAffiliation, affiliation_node))\n",
    "\n",
    "        # look for the field EMAIL:\n",
    "        email = None\n",
    "        # TODO: the email address actually belongs into the affiliation section, but we'll leave it directly in the contribution node for now:\n",
    "        if record.find(\"EMAIL\") is not None:\n",
    "            # get the email address from the EMAIL field, replacing spaces with underscores (common problem in urls in star) and adding a \"mailto:\" prefix:\n",
    "            email = \"mailto:\" + record.find(\"EMAIL\").text.strip().replace(\" \", \"_\")\n",
    "            # email = \"mailto:\" + record.find(\"EMAIL\").text.strip()\n",
    "            # if there is no EMID and the contribution_counter is 1 (i.e. if this is the first loop), add the email to the person node:\n",
    "            if record.find(\"EMID\") is None and contribution_counter == 1:\n",
    "                records_bf.add((contribution_node, MADS.email, URIRef(email)))\n",
    "            # else match the existing EMID field to the personname:\n",
    "            elif record.find(\"EMID\") is not None and record.find(\"EMID\").text.strip() == personname:\n",
    "                records_bf.add((contribution_node, MADS.email, URIRef(email))) \n",
    "                \n",
    "            \n",
    "        role = None\n",
    "        # check if there is a role in the AUP field:\n",
    "        if person.text.strip().find(\"|f \") > -1:\n",
    "            # save that text in a variable:\n",
    "            role = person.text.strip().split(\"|f\")[1].strip().split(\"|\")[0].strip()\n",
    "            # add it to the contribution node:\n",
    "            records_bf.add((contribution_node, BF.role, add_bf_contributor_person_role(role)))\n",
    "        # if there isn't, the role is \"AU\" by default:\n",
    "        else:\n",
    "            records_bf.add((contribution_node, BF.role, add_bf_contributor_person_role(\"AU\")))\n",
    "\n",
    "        ## --- Add the contribution node to the work node:\n",
    "        records_bf.add((work_uri, BF.contribution, contribution_node))\n",
    "        # add the person node to the contribution node as a contributor:\n",
    "        records_bf.add((contribution_node, BF.agent, person_node))    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Function: Create Topics, Weighted Topics and Classifications from CT, SH\n",
    "\n",
    "Maybe try lookup with Skosmos?\n",
    "\n",
    "Use this scheme:\n",
    "\n",
    "```turtle\n",
    "<Work> a bf:Work;\n",
    "    bf:subject [a bf:Topic, pxc:WeightedTopic, skos:Concept; # # topic, weighted\n",
    "        owl:sameAs <https://w3id.org/zpid/vocabs/terms/35365>;\n",
    "        rdfs:label \"Ontologies\"@en, \"Ontologien\"@de;\n",
    "        bf:source <https://w3id.org/zpid/vocabs/terms>;\n",
    "    ];\n",
    "    bf:subject [a bf:Topic, skos:Concept; # a non-weighted topic\n",
    "        owl:sameAs <https://w3id.org/zpid/vocabs/terms/60135>;\n",
    "        rdfs:label \"Semantic Networks\"@en, \"Semantische Netzwerke\"@de;\n",
    "        bf:source <https://w3id.org/zpid/vocabs/terms>;\n",
    "    ];\n",
    "    # PSYNDEX subject heading classification\n",
    "    bf:classification [ a bf:Classification, pxc:SubjectHeading, skos:Concept;\n",
    "        rdfs:label \"Professional Psychological & Health Personnel Issues\"@en;\n",
    "        bf:code \"3400\";\n",
    "        owl:sameAs <https://w3id.org/zpid/vocabs/class/3400>;\n",
    "        bf:source <https://w3id.org/zpid/vocabs/class>;\n",
    "    ].\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Function: Create nodes for Population Age Group (AGE) and Population Location (PLOC)\n",
    "\n",
    "Use this scheme:\n",
    "\n",
    "```turtle\n",
    "<Work> \n",
    "# age group study is about/sample was from:\n",
    "    bflc:demographicGroup [a bflc:DemographicGroup, pxc:AgeGroup, skos:Concept;\n",
    "        rdfs:label \"Adulthood\"@en, \"Erwachsenenalter\"@de;\n",
    "        owl:sameAs <https://w3id.org/zpid/vocabs/age/adulthood>;\n",
    "        bf:source <https://w3id.org/zpid/vocabs/age/AgeGroups>; \n",
    "    ];\n",
    "    # population location: \n",
    "    bf:geographicCoverage [a bf:GeographicCoverage, pxc:PopulationLocation, skos:Concept;\n",
    "        rdfs:label \"Germany\"@en;\n",
    "        owl:sameAs <countries/ger>;\n",
    "    ].\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create nodes for PRREG (linked Preregistration Works)\n",
    "\n",
    "Field PRREG can occur multiple times per record (0..n). \n",
    "It contains a link and/or DOI to a preregistration document. \n",
    "\n",
    "Possible subfields:\n",
    "- |u URL linking to the document\n",
    "- |d DOI for the document\n",
    "- |i additional Info text\n",
    "\n",
    "There are many errors we could catch here. \n",
    "- [x] Most importantly, we can replace any \" \" with \"_\" in the |u.\n",
    "- [x] Also, |d should contain pure DOIs with prefixes, so they should start with \"10.\" If they don't, remove any prefixes to make a \"pure\" DOI.\n",
    "- [x] remove or ignore any empty subfields that may exist (|u, |d, |i)\n",
    "\n",
    "Example:\n",
    "\n",
    "```turtle\n",
    "<https://w3id.org/zpid/pub/work/0003> a bf:Work; \n",
    "    bflc:relationship \n",
    "    [\n",
    "        a bflc:Relationship;\n",
    "        bflc:relation relations:hasPreregistration;\n",
    "        bf:note [a bf:Note; rdfs:label \"Australian Sample\"];\n",
    "        bf:supplement # may change, not sure?\n",
    "        [\n",
    "            a bf:Work, bf:Text; \n",
    "            bf:genreForm genres:preregistration; \n",
    "            bf:content content:text;\n",
    "            bf:hasInstance \n",
    "            [\n",
    "                a bf:Instance;\n",
    "                bf:electronicLocator <https://osf.io/prereg1>;\n",
    "                bf:identifier [a bf:Identifier, bf:Doi; rdf:value \"10.123code003\"];\n",
    "                # add bf:media \"computer\" from rda media types\n",
    "                bf:media <http://rdvocab.info/termList/RDAMediaType/1003>;\n",
    "                # bf:carrier \"online resource\" from rda vocabulary\n",
    "                bf:carrier <http://rdvocab.info/termList/RDACarrierType/1018>;\n",
    "            ]\n",
    "        ] \n",
    "    ]\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build the nodes for preregistration links\n",
    "def get_bf_preregistrations(work_uri, record):\n",
    "    # get the preregistration link from the field PREREG:\n",
    "    preregistration_link = \"\"\n",
    "    for prreg in record.findall(\"PRREG\"):\n",
    "        # build a blank node for bflc:Relationship:\n",
    "        preregistration_node = BNode()\n",
    "        records_bf.add((preregistration_node, RDF.type, BFLC.Relationship))\n",
    "        # add a blank node for the Work we will link:\n",
    "        preregistration_work_node = BNode()\n",
    "        records_bf.add((preregistration_work_node, RDF.type, BF.Work))\n",
    "        # make the work a bf:Text, because preregistrations always are:\n",
    "        records_bf.add((preregistration_work_node, RDF.type, BF.Text))\n",
    "        # add a bf:content of <https://w3id.org/zpid/vocabs/contenttypes/> to the work:\n",
    "        records_bf.add((preregistration_work_node, BF.content, URIRef(\"https://w3id.org/zpid/vocabs/contenttypes/text\")))\n",
    "        # add a bf:genreForm of <https://w3id.org/zpid/vocabs/genres/preregistration>:\n",
    "        records_bf.add((preregistration_work_node, BF.genreForm, URIRef(\"https://w3id.org/zpid/vocabs/genres/preregistration\")))\n",
    "        # add an instance to hang bf:electronicLocator and bf:Doi on:\n",
    "        preregistration_instance_node = BNode()\n",
    "        records_bf.add((preregistration_instance_node, RDF.type, BF.Instance))\n",
    "        # add a bf:media of <http://rdvocab.info/termList/RDAMediaType/1003> (computer) to the instance, because preregistrations are always online (they have a url or doi, after all):\n",
    "        records_bf.add((preregistration_instance_node, BF.media, URIRef(\"http://rdvocab.info/termList/RDAMediaType/1003\")))\n",
    "        # and also a bf:carrier <http://rdvocab.info/termList/RDACarrierType/1018> (online resource):\n",
    "        records_bf.add((preregistration_instance_node, BF.carrier, URIRef(\"http://rdvocab.info/termList/RDACarrierType/1018\"))) \n",
    "        # add the instance to the work:\n",
    "        records_bf.add((preregistration_work_node, BF.hasInstance, preregistration_instance_node))\n",
    "        # add the work to the relationship:\n",
    "        records_bf.add((preregistration_node, BF.supplement, preregistration_work_node))\n",
    "        # add a bflc:relation of <<https://w3id.org/zpid/vocabs/relations/hasPreregistration> to the relationship:\n",
    "        records_bf.add((preregistration_node, BFLC.relation, URIRef(\"https://w3id.org/zpid/vocabs/relations/hasPreregistration\")))\n",
    "        # get a preregistration link from the PREREG subfield |u\n",
    "        # but only if a |u exists:\n",
    "        if prreg.text.strip().find(\"|u\") > -1:\n",
    "            preregistration_link = prreg.text.strip().split(\"|u\")[1].strip().split(\"|\")[0].strip()\n",
    "            # replace any spaces in the link with underscores:\n",
    "            preregistration_link = preregistration_link.replace(\" \", \"_\")\n",
    "            # add a bf:electronicLocator to the instance:\n",
    "            records_bf.add((preregistration_instance_node, BF.electronicLocator, URIRef(preregistration_link)))\n",
    "\n",
    "        # get a DOI from the PREREG subfield |d, if it exists:\n",
    "        if prreg.text.find(\"|d\") > -1:\n",
    "            preregistration_doi = prreg.text.strip().split(\"|d\")[1].strip().split(\"|\")[0].strip()\n",
    "            # if this string is not empty:\n",
    "            if preregistration_doi !=\"\":\n",
    "                # check if it is even a DOI at all (i.e. starts with \"10.\"):\n",
    "                # make a regex pattern for dois that says they must start with \"10.\":\n",
    "                doi_pattern = re.compile(r'^10\\..*')\n",
    "                # if the string matches the pattern, keep it:\n",
    "                if doi_pattern.match(preregistration_doi):\n",
    "                    pass\n",
    "                # if it has anything before the \"10.\", remove thet part that comes before the \"10.\", but do keep \"10.\" any anything after it:\n",
    "                else:\n",
    "                    preregistration_doi = re.sub(r'^.*10\\.', '10.', preregistration_doi)\n",
    "\n",
    "                # add a bf:Doi to the instance using bf:identidiedBy and a blank node for the identifier (bf:Doi):\n",
    "                preregistration_doi_node = BNode()\n",
    "                records_bf.add((preregistration_doi_node, RDF.type, BF.Identifier))\n",
    "                records_bf.add((preregistration_doi_node, RDF.type, BF.Doi))\n",
    "                records_bf.add((preregistration_doi_node, RDF.value, Literal(preregistration_doi)))\n",
    "                records_bf.add((preregistration_instance_node, BF.identifiedBy, preregistration_doi_node))\n",
    "                # also generate a doi url and add it as a schema:sameAs link:\n",
    "                preregistration_doi_url = \"https://doi.org/\" + preregistration_doi\n",
    "                records_bf.add((preregistration_instance_node, SCHEMA.sameAs, URIRef(preregistration_doi_url)))\n",
    "            \n",
    "            # get note from the PREREG subfield |i, if it exists\n",
    "            if prreg.text.find(\"|i\") > -1:\n",
    "                preregistration_note = prreg.text.strip().split(\"|i\")[1].strip().split(\"|\")[0].strip()\n",
    "                if preregistration_note !=\"\":\n",
    "                    # create a bf:Note node:\n",
    "                    preregistration_note_node = BNode()\n",
    "                    records_bf.add((preregistration_note_node, RDF.type, BF.Note))\n",
    "                    # add the note to the note node via rdfs:label:\n",
    "                    records_bf.add((preregistration_note_node, RDFS.label, Literal(preregistration_note)))\n",
    "                    # add the note to the relationship node via bf:note:\n",
    "                    records_bf.add((preregistration_node, BF.note, preregistration_note_node))\n",
    "\n",
    "\n",
    "        # add preregistration_node to work:\n",
    "        records_bf.add((work_uri, BFLC.relationship, preregistration_node))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loop!\n",
    "## Creating the Work and Instance uris and adding other triples via functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uris and types for Bibframe profile\n",
    "\n",
    "We want two URIs, since we split the Records into (at first) one work and one instance, which will be linked together.\n",
    "We also say one will be a (rdf:type) bf:Work and the other bf:Instance.\n",
    "Then we print all these triples into a file for the bibframe profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93440 triples\n"
     ]
    }
   ],
   "source": [
    "# print(len(root.findall(\"Record\")))\n",
    "\n",
    "\n",
    "for record in root.findall(\"Record\"):\n",
    "\n",
    "    # get the DFK identifier from the record:\n",
    "    dfk = record.find(\"DFK\").text\n",
    "\n",
    "    # create a URI for the work and the instance and give them their correct bf classes:\n",
    "    work_uri = WORKS[dfk]\n",
    "    records_bf.add((work_uri, RDF.type, BF.Work))\n",
    "    instance_uri = INSTANCES[dfk]\n",
    "    records_bf.add((instance_uri, RDF.type, BF.Instance))\n",
    "\n",
    "    # connect work and instance via bf:instanceOf and bf:hasInstance:\n",
    "    records_bf.add((instance_uri, BF.instanceOf, work_uri))\n",
    "    records_bf.add((work_uri, BF.hasInstance, instance_uri))\n",
    "\n",
    "    # add an identifier bnode to the work using a function:\n",
    "    records_bf.add((instance_uri, BF.identifiedBy, get_bf_identifier_dfk(instance_uri, dfk)))\n",
    " \n",
    "\n",
    "    # get field TI and add as title node:\n",
    "    records_bf.add((instance_uri, BF.title, get_bf_title(instance_uri, record)))\n",
    "\n",
    "    # get work language from LA\n",
    "    records_bf.add((work_uri, BF.language, get_work_language(record)))\n",
    "\n",
    "    # get TIUE field and add as translated title node:\n",
    "    # but only if the field exists!\n",
    "    if record.find(\"TIUE\") is not None and record.find(\"TIUE\").text != \"\":\n",
    "        records_bf.add((instance_uri, BF.title, get_bf_translated_title(instance_uri, record)))\n",
    "\n",
    "\n",
    "    # get and add contributors:\n",
    "    # records_bf.add((work_uri, BF.contribution, add_bf_contributor_person(record)))\n",
    "    add_bf_contributor_person(work_uri, record)\n",
    "    # get toc, if it exists:\n",
    "    get_bf_toc(work_uri, record)\n",
    "    \n",
    "    # get and add main/original abstract:\n",
    "    # note: somehow not all records have one!\n",
    "    if record.find(\"ABH\") is not None:\n",
    "        records_bf.add((work_uri, BF.summary, get_bf_abstract(work_uri, record)))\n",
    "\n",
    "    # get and add main/original abstract:\n",
    "    # note: somehow not all records have one!\n",
    "    if record.find(\"ABN\") is not None:\n",
    "        records_bf.add((work_uri, BF.summary, get_bf_secondary_abstract(work_uri, record)))\n",
    "\n",
    "    # get and add preregistration links:\n",
    "    get_bf_preregistrations(work_uri, record)\n",
    "\n",
    "\n",
    "# print all the resulting triples:\n",
    "records_bf.serialize(\"ttl-data/bibframe_records.ttl\", format=\"turtle\")\n",
    "records_bf.serialize(\"ttl-data/bibframe_records.jsonld\", format=\"json-ld\")\n",
    "print(len(records_bf), \"triples\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uris and types for simplified profile (schema-org)\n",
    "\n",
    "For the simplified profile, we only need one entity per record (for now) and we give it the class schema:CreativeWork.\n",
    "Then we print the resulting triples into a separate file for the simplified profile that mostly uses schema.org properties and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(len(root.findall(\"Record\")))\n",
    "\n",
    "# for record in root.findall(\"Record\"):\n",
    "#     # get the DFK identifier from the record:\n",
    "#     dfk = record.find(\"DFK\").text\n",
    "\n",
    "#     # create a URI for the work by attaching the dfk to the works namespace and \n",
    "#     # then give it the correct schema.org class:\n",
    "#     work_uri = WORKS[dfk]\n",
    "#     records_schema.add((work_uri, RDF.type, SCHEMA.CreativeWork))\n",
    "\n",
    "#     # get work language from LA\n",
    "#     records_schema.add((work_uri, SCHEMA.inLanguage, get_work_language(record)))\n",
    "\n",
    "\n",
    "# records_schema.serialize(\"ttl-data/schema_records.jsonld\", format=\"json-ld\")\n",
    "# # records_schema.serialize(\"ttl-data/schema_records.ttl\", format=\"turtle\")\n",
    "# print(len(records_schema), \"triples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.10.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
